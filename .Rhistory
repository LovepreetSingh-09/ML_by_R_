# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))}
# So we need to check mean absolute error
MAE=function(act,pred){return mean(abs(act-pred))}
# So we need to check mean absolute error
MAE<-function(act,pred){
return mean(abs(act-pred))
}
MAE <- function(actual, predicted) {
mean(abs(actual - predicted))
}
# So we need to check mean absolute error
MAE<-function(act,pred){
mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
mean(abs(act-pred))
}
MAE(w_test$quality,pred)
library(RWeka)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
hist(data$quality,col='sky blue')
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
model=MSP(quality~.,data=w_train)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
m.m5p=M5P(quality~.,data=w_train)
summary(m.m5p)
m.m5p
pred=predict(m.m5p,w_test)
m.m5p <- M5P(quality ~ ., data = w_train)
m.m5p
summary(m.m5p)
install.packages("neuralnet")
library(neuralnet)
data=read.csv('concrete.txt')
str(data)
summary(data)
(x-min(x))/(max(x)-min(x))
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(norm,data))
concrete
concrete=as.data.frame(lapply(data,norm))
concrete
data=na.omit(data)
str(data)
summary(data)
concrete=as.data.frame(lapply(data,norm))
concrete
summary(concrete)
v=length(concrete)/0.8
v=length(concrete$cement)/0.8
split=split(length(concrete$cement),v)
set.seed(12345)
v=as.integer(length(concrete$cement)/0.8)
split=split(length(concrete$cement),v)
split
sp=sample(length(concrete$cement),v)
v=as.integer(length(concrete$cement)*0.8)
sp=sample(length(concrete$cement),v)
split
sp
# Generating Random Numbers
set.seed(1)
sd=sample(1000,900)
sd
train=concrete[sd:,]
train=concrete[sp:,]
v=as.integer(length(concrete$cement)*0.8)
sp=sample(length(concrete$cement),v)
sp
train=concrete[sp:,]
train=concrete[sp:]
train=concrete[c(sp):,]
train=concrete[sd,]
length(sp)
train=concrete[sd,]
test=concrete[-sd,]
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
length(sp)
train=concrete[sd,]
test=concrete[-sd,]
test=concrete[-sd,]
str(test)
train=concrete[sp,]
test=concrete[-sp,]
str(test)
summary(concrete)
library(neuralnet)
model=neuralnet(strength~.,train)
plot(model)
model=neuralnet(strength~.,train,hidden=1)
model
summary(model)
str(model)
model # Shows each weight and intercept/bias
pred=compute(model,test)
pred
plot(pred)
summary(pred)
str(pred)
cor(pred,test$strength)
pred=compute(model,test)
cor(pred,test$strength)
pred$neurons
pred$net.result
cor(pred$net.result,test$strength)
# Make neural network with 5 hidden layers
model=neuralnet(strength~.,train,hidden=5)
summary(model)
model
plot(model)
pred=compute(model,test)
cor(pred,test$strength)
cor(pred$net.result,test$strength)
# Make neural network with 5 hidden nodes
model=neuralnet(strength~.,train,hidden=4)
model
plot(model)
pred=compute(model,test)
cor(pred$net.result,test$strength)
plot(model)
pred=compute(model,test)
cor(pred$net.result,test$strength)
letters <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml11/letterdata.csv")
letters <- read.csv("letters_data")
letters <- read.csv("letters_data.txt")
str(letters)
library(e1071)
letters <- read.csv("letters_data.txt")
str(letters)
install.packages("ksvm")
install.packages("kernlab")
library(kernlab)
train=letters[1:16000,]
test=letters[16001:20000,]
model=ksvm(letter~.,train,kernel='vanilladot',C=1)
model
summary(model)
l=test$letter
pred=predict(model,test)
head(test$letter)
table(pred,test$letter)
# For checking confusion matrix
v=table(pred,test$letter)
table=table(v)
table
# For checking confusion matrix
v=pred==test$letter
table=table(v)
table
prop.table(table)
model=ksvm(letter~.,train,kernel='rbfdot',C=1)
pred=predict(model,test)
arg=pred==test$letter
prop.table(arg)
prop.table(table(arg)
prop.table(table(arg)
prop.table(table(arg))
install.packages("arules")
library(arules)
groceries=read.transactions('groceries.txt',sep=',')
summary(groceries)
str(groceries)
inspect(groceries[1:3])
groceries
itemFrequency(groceries)
itemFrequency(groceries[:,1:3])
itemFrequency(groceries[,1:3])
itemFrequency(groceries,topN=5)
itemFrequencyPlot(groceries,topN=5)
itemFrequencyPlot(groceries,support=0.1)
itemFrequencyPlot(groceries,topN=10)
image(groceries[1:5])
# visualize sparse matrix
image(groceries[1:5])
# sparse matrix of 100 random purchases
image(sample(groceries,100))
gro=apriori(groceries,parameter=list(support=0.006,confidence=0.7,minlen=2))
inspecct(gro[1:5])
inspect(gro[1:5])
gro=apriori(groceries,parameter=list(support=0.006,confidence=0.30,minlen=2))
summary(gro[1:5])
summary(gro)
gro
inspect(gro[1:3])
inspect(gro,by='lift')
inspect(sort(gro,by='lift')[1:5])
berry=subset(gro,items %in% berry)
berries=subset(gro,items %in% berries)
berries=subset(gro,items %in% 'berries')
inspect(berries)
fruits=subset(gro,items %in% 'fruits')
fruits=subset(gro,items %in% 'fruit')
fruits=subset(gro,items %ina% 'fruit')
fruits=subset(gro,items %pin% 'fruit')
inspect(fruits)
write(gro,'groceries.csv',sep=',',quote=TRUE,row.names=FALSE)
open('groceries.csv')
groc=read.csv('groceries.csv')
groc
str(groc)
summary(groc)
gr=as(gro,data.frame)
gr=as(gro,'data.frame')
str(gr)
# The rules column in data frame is converted into factors
str(gr)
library(stats)
interest=read.csv('sns.txt')
str(interest)
summary(interest)
source('~/.active-rstudio-document')
table(teens$gender,useNA='ifany')
summary(teens$age)
# Teens age can not be too small or too large so, limit it to 13-20 years
teens$age=ifelse(teens$age>13 & teens$age<20,teens$age,NA)
summary(teens$age)
# We can use dummy coding on gender
# We won't creatw variable for M becoz 0 to Female and no_gender means M
teens$female=ifelse(teens$gender=='F',1,0)
teens$no_gender=ifelse(is.na(teens$gender),1,0)
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
# We can use dummy coding on gender
# We won't creatw variable for M becoz 0 to Female and no_gender means M
teens$female=ifelse(teens$gender & (!is.na(teens$gender))=='F',1,0)
# We can use dummy coding on gender
# We won't creatw variable for M becoz 0 to Female and no_gender means M
teens$female=ifelse(teens$gender=='F' & (!is.na(teens$gender)),1,0)
table(teens$female,useNA='ifany')
table(teens$gender,useNA = TRUE)
table(teens$gender,useNA = 'ifany')
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
mean(teens$age)
mean(teens$age,na.rm=TRUE)
aggregate(age~gradyear,teens,mean,rm.na=TRUE)
# We can use ave() to store the information into our dataset
ave_age=ave(age~gradyear,teens,mean,rm.na=TRUE)
# We can use ave() to store the information into our dataset
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,rm.na=TRUE))
ave_age[1:5]
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# We can use ave() to store the information into our dataset
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,na.rm=TRUE))
ave_age[1:5]
ave_age[29995:30000]
summary(ave_age)
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# While we have to find the interest so, exclude the person info 1 to 4 columns
interest=teens[5:40]
# Standardize the data before training:
interest_=as.data.frame(lapply(interest,scale))
interst=kmeans(interest_,5)
model=kmeans(interest_,5)
model$centers
model$size
model$cluster
# Add cluster label column in the original data set
teens$cluster=model$cluster
teens[1:10,c('gradyear','age','gender','friends','cluster')]
aggreggate(age~cluster,teens,mean)
aggregate(age~cluster,teens,mean)[1:10]
aggregate(age~cluster,teens,mean)
aggregate(gender~cluster,teens,mean)
aggregate(female~cluster,teens,mean)
aggregate(friends~cluster,teens,mean)
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
install.packages("caret")
source('~/R/ML_by_R/Data_Analysis.R')
# checking the relation between the model and the color
CrossTable(x=usedcars$model,y=conservative)
library(gmodels)
source('~/R/ML_by_R/Data_Analysis.R')
# checking the relation between the model and the color
CrossTable(x=usedcars$model,y=conservative)
library(caret)
# kappa statistic :-
# It adjusts accuracy by accounting for the possibility of a correct prediction by chance alone.
# It is (pr(a)-pr(e))/1-pr(e)
pr_a=0.865+0.109 # True Positive and True Negative proportions
pr_e=0.869*0.888 + 0.132*0.112 # (TP+FP)*(TP+FN) + (TN+FN)*(TN*FP)
kap=(pr_a-pr_e)/(1-pr_e)
kap
install.packages("vcd")
library(vcd)
# Kappa funvtion in vcd package
Kappa(c(0.865,0.03),c(0.023,0.109))
# Kappa funvtion in vcd package
Kappa(table(c(0.865,0.03),c(0.023,0.109)))
install.packages("irr")
library(irr)
# Kappa2 in irr package
Kappa(as.data.frame(c(0.865,0.03),c(0.023,0.109)))
table
# Kappa2 in irr package
Kappa2(table(c(0.865,0.03),c(0.023,0.109)))
# Kappa2 in irr package
Kappa2(as.data.frame(c(0.865,0.03),c(0.023,0.109)))
# Kappa2 in irr package
kappa2(as.data.frame(c(0.865,0.03),c(0.023,0.109)))
# Kappa funvtion in vcd package
Kappa(table(c(0.865,0.03),c(0.023,0.109)))
# Sensitivity is TPR
# Specificity is TNR
help("sensitivity")
help("specificity")
# Kappa2 in irr package
help(kappa2)
# Sensitivity is TPR
# Specificity is TNR
help(sensitivity)
# Here we need to pass the actual and predicted vectors
# Kappa funvtion in vcd package
help(Kappa)
kappa2(as.data.frame(c(1,0),c(0,1)))
help("posPredValue")
install.packages("ROCR")
library(ROCR)
help("prediction")
help('performance')
abline(a=0,b=1,lwd=2)
abline(a=0,b=0.2,lwd=2)
abline(a=0,b=0.11,lwd=2)
source('~/R/ML_by_R/Loan_model.R')
random_ids=order(runif(1000))
random_ids
credit_train <- credit[random_ids[1:500], ]
credit_validate <- credit[random_ids[501:750], ]
credit_test <- credit[random_ids[751:1000], ]
library(caret)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=CreateDataPartition(credit$default,p=0.75,list=FALSE)
library(caret)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=CreateDataPartition(credit$default,p=0.75,list=FALSE)
createDataPartition(credit$default,p=0.75,list=FALSE)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=CreateDataPartition(credit$default,p=0.75,list=FALSE)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=createDataPartition(credit$default,p=0.75,list=FALSE)
credit_train=credit[it,]
credit_test=credit[-it,]
length(it)
str(folds)
# using KFold Cross-Validation
folds=createFolds(credit$default,k=10)
str(folds)
library(irr)
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
cv_results=lapply(folds,function(x){
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kappa=kappa2(data.frame(credit_actual,credit_pred))$value
return kappa
})
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kappa=kappa2(data.frame(credit_actual,credit_pred))$value
return kappa
})
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kappa=kappa2(data.frame(credit_actual,credit_pred))$value
return kappa })
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return kp
})
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp) })
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
str(credit)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
# using KFold Cross-Validation
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
# kaapa2 from irr package
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
# using KFold Cross-Validation
set.seed(123)
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
# kaapa2 from irr package
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
# use unlist function to unlist
mean(cv_results)
# use unlist function to unlist
mean(mean(cv_results))
# use unlist function to unlist
mean(unlist(cv_results))
