inspect(gro[1:3])
inspect(gro,by='lift')
inspect(sort(gro,by='lift')[1:5])
berry=subset(gro,items %in% berry)
berries=subset(gro,items %in% berries)
berries=subset(gro,items %in% 'berries')
inspect(berries)
fruits=subset(gro,items %in% 'fruits')
fruits=subset(gro,items %in% 'fruit')
fruits=subset(gro,items %ina% 'fruit')
fruits=subset(gro,items %pin% 'fruit')
inspect(fruits)
write(gro,'groceries.csv',sep=',',quote=TRUE,row.names=FALSE)
open('groceries.csv')
groc=read.csv('groceries.csv')
groc
str(groc)
summary(groc)
gr=as(gro,data.frame)
gr=as(gro,'data.frame')
str(gr)
# The rules column in data frame is converted into factors
str(gr)
library(stats)
interest=read.csv('sns.txt')
str(interest)
summary(interest)
source('~/.active-rstudio-document')
table(teens$gender,useNA='ifany')
summary(teens$age)
# Teens age can not be too small or too large so, limit it to 13-20 years
teens$age=ifelse(teens$age>13 & teens$age<20,teens$age,NA)
summary(teens$age)
# We can use dummy coding on gender
# We won't creatw variable for M becoz 0 to Female and no_gender means M
teens$female=ifelse(teens$gender=='F',1,0)
teens$no_gender=ifelse(is.na(teens$gender),1,0)
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
# We can use dummy coding on gender
# We won't creatw variable for M becoz 0 to Female and no_gender means M
teens$female=ifelse(teens$gender & (!is.na(teens$gender))=='F',1,0)
# We can use dummy coding on gender
# We won't creatw variable for M becoz 0 to Female and no_gender means M
teens$female=ifelse(teens$gender=='F' & (!is.na(teens$gender)),1,0)
table(teens$female,useNA='ifany')
table(teens$gender,useNA = TRUE)
table(teens$gender,useNA = 'ifany')
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
mean(teens$age)
mean(teens$age,na.rm=TRUE)
aggregate(age~gradyear,teens,mean,rm.na=TRUE)
# We can use ave() to store the information into our dataset
ave_age=ave(age~gradyear,teens,mean,rm.na=TRUE)
# We can use ave() to store the information into our dataset
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,rm.na=TRUE))
ave_age[1:5]
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# We can use ave() to store the information into our dataset
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,na.rm=TRUE))
ave_age[1:5]
ave_age[29995:30000]
summary(ave_age)
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# While we have to find the interest so, exclude the person info 1 to 4 columns
interest=teens[5:40]
# Standardize the data before training:
interest_=as.data.frame(lapply(interest,scale))
interst=kmeans(interest_,5)
model=kmeans(interest_,5)
model$centers
model$size
model$cluster
# Add cluster label column in the original data set
teens$cluster=model$cluster
teens[1:10,c('gradyear','age','gender','friends','cluster')]
aggreggate(age~cluster,teens,mean)
aggregate(age~cluster,teens,mean)[1:10]
aggregate(age~cluster,teens,mean)
aggregate(gender~cluster,teens,mean)
aggregate(female~cluster,teens,mean)
aggregate(friends~cluster,teens,mean)
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
install.packages("caret")
source('~/R/ML_by_R/Data_Analysis.R')
# checking the relation between the model and the color
CrossTable(x=usedcars$model,y=conservative)
library(gmodels)
source('~/R/ML_by_R/Data_Analysis.R')
# checking the relation between the model and the color
CrossTable(x=usedcars$model,y=conservative)
library(caret)
# kappa statistic :-
# It adjusts accuracy by accounting for the possibility of a correct prediction by chance alone.
# It is (pr(a)-pr(e))/1-pr(e)
pr_a=0.865+0.109 # True Positive and True Negative proportions
pr_e=0.869*0.888 + 0.132*0.112 # (TP+FP)*(TP+FN) + (TN+FN)*(TN*FP)
kap=(pr_a-pr_e)/(1-pr_e)
kap
install.packages("vcd")
library(vcd)
# Kappa funvtion in vcd package
Kappa(c(0.865,0.03),c(0.023,0.109))
# Kappa funvtion in vcd package
Kappa(table(c(0.865,0.03),c(0.023,0.109)))
install.packages("irr")
library(irr)
# Kappa2 in irr package
Kappa(as.data.frame(c(0.865,0.03),c(0.023,0.109)))
table
# Kappa2 in irr package
Kappa2(table(c(0.865,0.03),c(0.023,0.109)))
# Kappa2 in irr package
Kappa2(as.data.frame(c(0.865,0.03),c(0.023,0.109)))
# Kappa2 in irr package
kappa2(as.data.frame(c(0.865,0.03),c(0.023,0.109)))
# Kappa funvtion in vcd package
Kappa(table(c(0.865,0.03),c(0.023,0.109)))
# Sensitivity is TPR
# Specificity is TNR
help("sensitivity")
help("specificity")
# Kappa2 in irr package
help(kappa2)
# Sensitivity is TPR
# Specificity is TNR
help(sensitivity)
# Here we need to pass the actual and predicted vectors
# Kappa funvtion in vcd package
help(Kappa)
kappa2(as.data.frame(c(1,0),c(0,1)))
help("posPredValue")
install.packages("ROCR")
library(ROCR)
help("prediction")
help('performance')
abline(a=0,b=1,lwd=2)
abline(a=0,b=0.2,lwd=2)
abline(a=0,b=0.11,lwd=2)
source('~/R/ML_by_R/Loan_model.R')
random_ids=order(runif(1000))
random_ids
credit_train <- credit[random_ids[1:500], ]
credit_validate <- credit[random_ids[501:750], ]
credit_test <- credit[random_ids[751:1000], ]
library(caret)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=CreateDataPartition(credit$default,p=0.75,list=FALSE)
library(caret)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=CreateDataPartition(credit$default,p=0.75,list=FALSE)
createDataPartition(credit$default,p=0.75,list=FALSE)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=CreateDataPartition(credit$default,p=0.75,list=FALSE)
# Using Stratified Kfold method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=createDataPartition(credit$default,p=0.75,list=FALSE)
credit_train=credit[it,]
credit_test=credit[-it,]
length(it)
str(folds)
# using KFold Cross-Validation
folds=createFolds(credit$default,k=10)
str(folds)
library(irr)
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
cv_results=lapply(folds,function(x){
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kappa=kappa2(data.frame(credit_actual,credit_pred))$value
return kappa
})
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kappa=kappa2(data.frame(credit_actual,credit_pred))$value
return kappa
})
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kappa=kappa2(data.frame(credit_actual,credit_pred))$value
return kappa })
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return kp
})
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp) })
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
str(credit)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
# using KFold Cross-Validation
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
# kaapa2 from irr package
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
# using KFold Cross-Validation
set.seed(123)
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
# kaapa2 from irr package
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
str(cv_results)
# use unlist function to unlist
mean(cv_results)
# use unlist function to unlist
mean(mean(cv_results))
# use unlist function to unlist
mean(unlist(cv_results))
source('~/R/ML_by_R/Concrete_Strength.R')
model
plot(model) # Now our SSE error is reduced to approx. 1 and steps has been increased
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
source('~/.active-rstudio-document')
?C5.0
?expand.grid
modelLookup(C5.0)
modelLookup('C5.0')
grid=expand.grid(.model='tree',.trials=c(1,5,15,15,30,35),.winnow='FALSE')
grid
set.seed(300)
m=train(default~.,credit,method='C5.0',metric='Kappa',trControl=ctrl,tuneGrid=grid)
# The trainControl() function is used to create a set of configuration options known
# as a control object, which guides the train() function.
ctrl=trainControl(method='cv',selectionFunction = 'oneSE',number = 10)
grid=expand.grid(.model='tree',.trials=c(1,5,15,15,30,35),.winnow='FALSE')
grid
set.seed(300)
m=train(default~.,credit,method='C5.0',metric='Kappa',trControl=ctrl,tuneGrid=grid)
str(credit)
m=train(default~.,credit,method='C5.0',metric='Kappa',trControl=ctrl,tuneGrid=grid)
m
m=train(default~.,data=credit,method='C5.0',metric='Kappa',trControl=ctrl,tuneGrid=grid)
install.packages("ipred")
install.packages("ipred")
install.packages("adabag")
install.packages("randomForest")
library(ipred)
source('~/.active-rstudio-document')
mybag <- bagging(default ~ ., data = credit, nbagg = 25)
credit_pred <- predict(mybag, credit)
> table(credit_pred, credit$default)
table(credit_pred, credit$default)
table(factor(credit_pred, credit$default))
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10)
train(default ~ ., data = credit, method = "treebag", trControl = ctrl)
modelLookup(mybag)
train(default ~ ., data = credit, method = "treebag", trControl = ctrl)
str(svmBag)
svmBag$fit()
svmBag$fit
# nBag=naive bayes, ctreeBag=decisiontree, nnetBag=nn
bagctrl <- bagControl(fit = svmBag$fit,
predict = svmBag$pred,
aggregate = svmBag$aggregate)
svmbag <- train(default ~ ., data = credit, "bag",
trControl = ctrl, bagControl = bagctrl)
svmbag
svmbag <- train(default ~ ., data = credit, "bag",
trControl = ctrl, bagControl = bagctrl)
m <- train(default ~ ., data = credit, method = "C5.0")
modelLookup('C5.0')
library(caret)
m <- train(default ~ ., data = credit, method = "C5.0")
credit$default=factor(sapply((credit$default),crop))
m <- train(default ~ ., data = credit, method = "C5.0")
p <- predict(m, credit)
table(p, credit$default)
m
head(predict(m,credit,'prob'))
# The trainControl() function is used to create a set of configuration options known
# as a control object, which guides the train() function.
ctrl=trainControl(method='cv',selectionFunction = 'oneSE',number = 10)
grid=expand.grid(.model='tree',.trials=c(1,5,15,15,30,35),.winnow='FALSE')
grid
set.seed(300)
m=train(default~.,data=credit,method='C5.0',metric='Kappa',trControl=ctrl,tuneGrid=grid)
m
source('~/.active-rstudio-document')
credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10)
# train of caret do CV of 10 folds by default
train(default ~ ., data = credit, method = "treebag", trControl = ctrl)
# svmBag of caret supplies 3 functions
str(svmBag)
svmBag$fit
# nBag=naive bayes, ctreeBag=decisiontree, nnetBag=nn
bagctrl <- bagControl(fit = svmBag$fit,
predict = svmBag$pred,
aggregate = svmBag$aggregate)
svmbag <- train(default ~ ., data = credit, "bag",
trControl = ctrl, bagControl = bagctrl)
library(adabag)
svmbag
svmbag <- train(default ~ ., data = credit, "nbag",
trControl = ctrl, bagControl = bagctrl)
svmbag <- train(default ~ ., data = credit, "bag",
trControl = ctrl, bagControl = bagctrl)
optim.na(credit)
na.omit(credit)
svmbag <- train(default ~ ., data = credit, "bag",
trControl = ctrl, bagControl = bagctrl)
credit=na.omit(credit)
credit=na.omit(credit)
svmbag <- train(default ~ ., data = credit, "bag",
trControl = ctrl, bagControl = bagctrl)
# AdaBoosting
# from adabag
m_adaboost=boosting(default~.,credit)
p_adaboost <- predict(m_adaboost, credit)
p_adaboost$confusion
# Boosting using Cross Validation
m_adaboost=boosting.cv(default~.,credit)
m_adaboost$confusion
library(vcd)
Kappa(m_adaboost$confusion)
# Random Forests
# from randomForest package
library(randomForest)
rf=randomForest(default~.,credit,ntree=500)
rf
# for validation from caret with 10 folds repeat for 10 times
ctrl=trainControl(method='repeatedcv',number=10,repeats=10)
modelLookup(rf)
# mtry is for no. of random selction of features which is sqrt(16)=4 by default
grid=expand.grid(.mtry=c(2,4,8,16))
# using train of caret
m_rf=train(default~.,credit,method='rf',tuneGrig=grid,trcontrol=ctrl,metric='Kappa')
# comparing with simple decision tree f C5.0
grid_c50 <- expand.grid(.model = "tree",.trials = c(10, 20, 30, 40),.winnow = "FALSE")
m_c50 <- train(default ~ ., data = credit, method = "C5.0", metric = "Kappa", trControl = ctrl, tuneGrid = grid_c50)
View(grid_c50)
m_rf
m_rf
m_c50
# train of caret do CV of 10 folds by default
train(default ~ ., data = credit, method = "treebag")
# train of caret do CV of 10 folds by default
train(default ~ ., data = credit, method = "treebag", trControl = ctrl)
source('~/R/ML_by_R/Concrete_Strength.R')
cor(pred$net.result,test$strength)
library(neuralnet)
library(MASS)
Boston
str(Boston)
data=Boston
hist(data$medv)
hist(data$medv)
head(data)
hist(data$medv)
hist(data$medv,col='sky blue')
maxvalue=apply(data,2,max)
maxvalue
minvalue=apply(data,2,min)
minvalue
dataf=as.data.frame(scale(data))
summary(dataf)
dataf=as.data.frame(scale(data,center=minvalue,scale=maxvalue-minvalue))
summary(dataf)
norm=function(x){ ((x-min(x))/(max(x)-min(x)))}
dt=lapply(data,norm)
summary(dt)
dt=as.data.frame(lapply(data,norm))
summary(dt)
summary(dataf)
ind=sample(1:nrow(data),400)
str(ind)
ind=sample(1:nrow(dataf),400)
ind=sample(1:nrow(dataf),400)
str(ind)
traindf=dataf
traindf=dataf[ind,]
str(traindf)
testdf=dataf[-ind,]
str(testdf)
allvars=colnames(data)
allvars=colnames(data)
allvars
predictors=allvar[!allvars %in% 'medv']
predictors=allvars[!allvars %in% 'medv']
predictors
predictors=paste(predictors,collapse = '+')
predictors
form=as.formula(paste('medv~',predictors,collapse='+'))
form
neuralmodel=neuralnet(formula=form,hidden=c(4,2),linear.output=T,data=traindf)
plot(neuralmodel)
source('~/R/ML_by_R/Association_Rules.R')
# The rules column in data frame is converted into factors
str(gr)
inspect(berries)
inspect(fruits)
summary(pred)
pred=predict(model,w_test)
summary(pred)
source('~/R/ML_by_R/White_Wine_Regression.R')
# The rules column in data frame is converted into factors
str(gr)
source('~/R/ML_by_R/Association_Rules.R')
# visualize sparse matrix
image(groceries[1:5])
# sparse matrix of 100 random purchases
image(sample(groceries,100))
# Frequency on the basis of conditions
itemFrequencyPlot(groceries,support=0.1)
itemFrequencyPlot(groceries,topN=10)
# visualize sparse matrix
image(groceries[1:5])
# sparse matrix of 100 random purchases
image(sample(groceries,100))
# Train model
gro=apriori(groceries,parameter=list(support=0.006,confidence=0.30,minlen=2))
gro
summary(gro)
help(support)
summary(groceries)
groceries
# First 3 purchases
inspect(groceries[1:3])
itemFrequency(groceries[,1:3])
# First 3 purchases
groceries[,1:3]
# First 3 purchases
inspect(groceries[,1:3])
itemFrequency(groceries[1:50,1:3])
# First 3 purchases
inspect(groceries[1:50,1:3])
itemFrequency(groceries[,1:3])
