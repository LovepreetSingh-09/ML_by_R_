gro
summary(gro)
help(support)
summary(groceries)
groceries
# First 3 purchases
inspect(groceries[1:3])
itemFrequency(groceries[,1:3])
# First 3 purchases
groceries[,1:3]
# First 3 purchases
inspect(groceries[,1:3])
itemFrequency(groceries[1:50,1:3])
# First 3 purchases
inspect(groceries[1:50,1:3])
itemFrequency(groceries[,1:3])
source('~/R/ML_by_R/Loan_model.R')
# Generating Random Numbers
set.seed(1)
sd=sample(1000,900)
library(C50)
library(caret)
library(gmodels)
library(irr)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
crop=function(x){x=ifelse(x>1,'Yes','No')}
# spliting data for training and testing
credit_train=credit[sd,-17]
credit_train_labels=factor(sapply((credit$default[sd]),crop))
str(credit_train)
str(credit_train_labels)
table(credit_train_labels)
credit_test=credit[-sd,-17]
str(credit_test)
credit_test_labels=factor(sapply(credit[-sd,17],crop))
table(credit_test_labels)
credit$default=factor(sapply((credit$default),crop))
# trails are the number of trees to be made for boosting. 1 by default.
# Costs is a matrix associated with the different types of errors. NULL by default.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
# model gives the depth or size, samples and features used
# Summary shows the rules by which tree is made of and the error rate.
model
summary(model)
# Evaluating the model
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=4)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Creating random no.
random_ids=order(runif(1000))
random_ids
library(rpart.plot)
library(rpart)
library(rpart.plot)
# trails are the number of trees to be made for boosting. 1 by default.
# Costs is a matrix associated with the different types of errors. NULL by default.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
# model gives the depth or size, samples and features used
# Summary shows the rules by which tree is made of and the error rate.
model
rpart.plot(model)
library(rpart)
library(rpart.plot)
iris
s=sample(150,100)
s
str(s)
iris_train=iris[s,]
iris_test=iris[-s,]
iris_train
iris_test
dtm=rpart(Species~.,iris_train,method='class')
dtm
plot(dtm)
text(dtm)
plot(dtm)
rpart.plot(dtm)
plot(dtm)
text(dtm)
plot(dtm)
rpart.plot(dtm)
rpart.plot(dtm,type=4,extra=101)
p=predict(dtm,iris_test,type='class')
p
table(iris_test[,5],p)
model=C5.0(Species~.,iris_train,method='class')
model
str(model)
predict(model,iris_test)
?C5.0
?rpart
p
table(iris_test[,5],p)
iris_test[,4]
iris_test[,5]
rpart.plot(credit_train,credit_train_labels)
View(credit_train)
dtm=rpart(credit_train,credit_train_labels)
dtm=rpart(credit_train,credit_train_labels)
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
a
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
library(psych)
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
str(insurance)
str(challanger)
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
str(challanger)
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
library(psych)
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
str(insurance)
str(challanger)
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
a=as.matrix(x)
reg(y = challanger$distress_ct, x = challanger[2:4])
str(challanger)
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
str(insurance)
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
str(challanger)
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
reg(y = challanger$distress_ct, x = challanger[c(1,3:5)])
challanger[c(1,3:5)]
reg(y = challanger$distress_ct, x = challanger[c(1,3:5)])
# Predicting Medical Expenses
str(insurance)
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
# Exploring relationship b/w all variables by pearson correlation
# The diagonal here will be 1 becoz of variables covariance with itself and its variance is same
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
# you can write . in place of all independent variables
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
library(caTools)
library(ggplot2)
library(gmodels)
getwd()
dataset=read.csv('Salary_Data.csv')
str(dataset)
split=sample.split(dataset$Salary,SplitRatio=0.7)
split=sample.split(dataset$Salary,SplitRatio=0.7)
# returns the split in True/False values
split
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
# Build model
model=lm(formula=Salary~YearsExperience,data=train_dataset)
model # shows intercept and weight value
summary(model)
coef(model)
ypred=predict(model,newdata=test_data)
ggplot()+geom_point(aes(x=train_dataset$YearsExperience,y=train_dataset$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience),y=predict(model,newdata=train_dataset),color='blue')+
ggtitle('Salary vs Experience')+
xlab('Years of Experience')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=test_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=test_data)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
rain
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
lab(x='Years',y='salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
labs(x='Years',y='salary')
maxvalue
library(neuralnet)
library(MASS)
# Dataset in MASS function
str(Boston)
data=Boston
hist(data$medv,col='sky blue')
head(data)
maxvalue=apply(data,2,max)
maxvalue
minvalue=apply(data,2,min)
minvalue
dataf=as.data.frame(scale(data,center=minvalue,scale=maxvalue-minvalue))
summary(dataf)
# It is sameas normalization
norm=function(x){ ((x-min(x))/(max(x)-min(x)))}
dt=as.data.frame(lapply(data,norm))
summary(dt)
ind=sample(1:nrow(dataf),400)
str(ind)
traindf=dataf[ind,]
ind=sample(1:nrow(dataf),400)
str(ind)
traindf=dataf[ind,]
str(traindf)
testdf=dataf[-ind,]
str(testdf)
allvars=colnames(data)
allvars
predictors=allvars[!allvars %in% 'medv']
predictors
# joining variable with + sign
predictors=paste(predictors,collapse = '+')
predictors
form=as.formula(paste('medv~',predictors,collapse='+'))
form
neuralmodel=neuralnet(formula=form,hidden=c(4,2),linear.output=T,data=traindf)
plot(neuralmodel)
library(rpart)
library(rpart)
library(RWeka)
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(at2))
sdr1
sdr2=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
sdr2
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
hist(data$quality,col='sky blue')
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
model=rpart(quality~.,w_train)
model
# The nodes having * can be used for predictions
summary(model)
# Visualizing Tree
rpart.plot(model,digits=4,type=3,fallen.leaves=TRUE,extra = 101)
pred=predict(model,w_test)
summary(pred)
summary(w_test$quality)
cor(pred,w_test$quality)
# So we need to check mean absolute error
MAE=function(act,pred){
mean(abs(act-pred))
}
MAE(w_test$quality,pred)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
library(neuralnet)
data=read.csv('concrete.txt')
# Exclude missing values
data=na.omit(data)
str(data)
summary(data)
data=read.csv('concrete.txt')
summary(data)
data=data(na.rm=TRUE)
summary(data)
summary(data)
data=read.csv('concrete.txt')
summary(data)
data=max(data,na.rm=TRUE)
summary(data)
data=read.csv('concrete.txt')
data=data[is.na(d)]
data=data[is.na(data)]
summary(data)
dsta=lapply(data,na.rm=TRUE)
data=read.csv('concrete.txt')
summary(data)
0 <- data[is.na(data)]
data[is.na(data)]=0
summary(data)
data=read.csv('concrete.txt')
str(data)
v=as.integer(length(concrete$cement)*0.75)
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
summary(data)
str(data)
(x-min(x))/(max(x)-min(x))
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
# Exclude missing values
data=na.omit(data)
str(data)
summary(data)
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
set.seed(12345)
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
length(sp)
train=concrete[sp,]
test=concrete[-sp,]
str(test)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=1)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3))
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3))
model # Shows each weight and intercept/bias
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,2))
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3,4))
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=2)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(2))
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
pred$net.result
cor(pred$net.result,test$strength) # 0.821
pred$net.result
pred$neurons
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
model # Shows each weight and intercept/bias
pred$neurons
pred$net.result
summary(data)
pred$net.result
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred=compute(model,test)
cor(pred$net.result,test$strength)
