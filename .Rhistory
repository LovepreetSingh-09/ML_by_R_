v=as.integer(length(concrete$cement)*0.75)
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
summary(data)
str(data)
(x-min(x))/(max(x)-min(x))
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
# Exclude missing values
data=na.omit(data)
str(data)
summary(data)
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
set.seed(12345)
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
length(sp)
train=concrete[sp,]
test=concrete[-sp,]
str(test)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=1)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3))
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3))
model # Shows each weight and intercept/bias
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,2))
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(3,3,4))
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=2)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(2))
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
pred$net.result
cor(pred$net.result,test$strength) # 0.821
pred$net.result
pred$neurons
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
model # Shows each weight and intercept/bias
pred$neurons
pred$net.result
summary(data)
pred$net.result
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred=compute(model,test)
cor(pred$net.result,test$strength)
library(kernlab)
letters <- read.csv("letters_data.txt")
str(letters)
train=letters[1:16000,]
test=letters[16001:20000,]
model=ksvm(letter~.,train,kernel='vanilladot',C=1)
model
pred=predict(model,test)
head(test$letter)
# For checking confusion matrix
v=pred==test$letter
table=table(v)
table
prop.table(table)
table(pred,test$letter)
# Confusion Matrix
table(pred,test$letter)
# improving model by rbf kernel
model=ksvm(letter~.,train,kernel='rbfdot',C=1)
# improving model by rbf kernel
?ksvm
letters <- read.csv("letters_data.txt")
library(kernlab)
letters <- read.csv("letters_data.txt")
str(letters)
train=letters[1:16000,]
test=letters[16001:20000,]
model=ksvm(letter~.,train,kernel='vanilladot',C=1)
model
pred=predict(model,test)
head(test$letter)
# For checking confusion matrix
v=pred==test$letter
table=table(v)
table
prop.table(table)
# Confusion Matrix
table(pred,test$letter)
# improving model by rbf kernel
?ksvm
model=ksvm(letter~.,train,kernel='rbfdot',C=1)
library(stats)
teens=read.csv('sns.txt')
str(teens)
# There are NA values in gender and age which means the missing values
summary(teens)
# There are almost 2800 NA values
table(teens$gender,useNA='ifany')
# almost 5000 NA in age
summary(teens$age)
# Teens age can not be too small or too large so, limit it to 13-20 years
teens$age=ifelse(teens$age>13 & teens$age<20,teens$age,NA)
summary(teens$age)
# We can use dummy coding on gender
# We won't create variable for M becoz 0 to Female and no_gender means M
# We need to apply !(is.na(teens$gender)) becoz otherwise NA values will appear in female variables also
teens$female=ifelse(teens$gender=='F' & (!is.na(teens$gender)),1,0)
teens$no_gender=ifelse(is.na(teens$gender),1,0)
table(teens$gender,useNA = 'ifany')
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
# only after extracting NA we can find the mean of a variable
mean(teens$age)
mean(teens$age,na.rm=TRUE)
# Mean age with respect to grad yeR
# aggregate gives the output in a data frame so it is difficult to merge it into our data
aggregate(age~gradyear,teens,mean,na.rm=TRUE)
# We can use ave() to store the information into our dataset
# It gives the vector of the output
# It has 30000 values which are mean of their respective gradyear
# We can use ave() to store the information into our dataset
# It gives the vector of the output
# It has 30000 values which are mean of their respective gradyear
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,na.rm=TRUE))
ave_age[29995:30000]
summary(ave_age)
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
teens$age=ifelse(is.na(teens$age),ave_age)
? ave()
ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# While we have to find the interest so, exclude the person info 1 to 4 columns
interest=teens[5:40]
# Standardize the data before training:
interest_=as.data.frame(lapply(interest,scale))
str(teens)
# Standardize the data before training:
interest_=as.data.frame(lapply(interest,scale))
# Train the model
set.seed(12345)
model=kmeans(interest_,5)
# cluster centers of each variable
model$centers
# Size or no. of elements in the clusters
model$size
# Cluster label of each data point of the training dataset
model$cluster
# Add cluster label column in the original data set
teens$cluster=model$cluster
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
# Average age based on each cluster
aggregate(age~cluster,teens,mean)
# Female proportion in each cluster
aggregate(female~cluster,teens,mean)
# Average no. of friends based on each cluster
aggregate(friends~cluster,teens,mean)
source('~/R/ML_by_R/SNS_Clustering.R')
teens$cluster=factor(teens$cluster,levels=c('One',"two",'three','four','five'))
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
# Add cluster label column in the original data set
teens$cluster=model$cluster
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
# Average age based on each cluster
aggregate(age~cluster,teens,mean)
# Female proportion in each cluster
aggregate(female~cluster,teens,mean)
# Average no. of friends based on each cluster
aggregate(friends~cluster,teens,mean)
teens$cluster=factor(teens$cluster,levels=c(1='One',2="two",3='three',4='four',5='five'))
teens$cluster=factor(teens$cluster,levels=c(1:'One',2="two",3='three',4='four',5='five'))
teens$cluster=factor(teens$cluster,levels=c(1:'One',2:"two",3:'three',4:'four',5:'five'))
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
library(psych)
library(ggplot2)
data=read.csv('challanger.txt')
str(data)
m=cov(data$temperature,data$distress_ct)/var(data$temperature)
m
b=mean(data$distress_ct)-m*(mean(data$temperature))
b
cor(data$temperature,data$distress_ct)
reg=function(x,y){
a=as.matrix(x)
a=cbind(Intercept=1,a)
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=data$temperature,y=data$distress_ct)
pairs(data[c('temperature','pressure','distress_ct')])
cor(data$temperature,data$distress_ct)
cor(data$pressure,data$distress_ct)
set.seed(123)
set.seed(123)
n=nrow(data)
sd=sample(n,as.integer(n*0.70))
sd
n
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
teens$cluster=factor(teens$cluster)
# Info of a person with its assigned cluster label
teens[1:10,c('gradyear','age','gender','friends','cluster')]
# Info of a person with its assigned cluster label
str(teens[1:10,c('gradyear','age','gender','friends','cluster')])
teens$cluster=factor(teens$cluster,levels=c('1':'One','2':"two",'3':'three','4':'four','5':'five'))
mapvalues(teens$cluster, from = c("1", "2"), to = c("two", "three"))
source('~/R/ML_by_R/SNS_Clustering.R')
source('~/R/ML_by_R/SNS_Clustering.R')
mapvalues(teens$cluster, from = c("1", "2",'3','4','5'), to = c('one',"two", "three",'four','five'))
reval(teens4cluster)
reval(teens$cluster)
levels(teens$cluster)
levels(teens$cluster) <- c('one',"two", "three",'four','five')
levels(teens$cluster)
teens$cluster
# Info of a person with its assigned cluster label
str(teens[1:10,c('gradyear','age','gender','friends','cluster')])
# Info of a person with its assigned cluster label
(teens[1:10,c('gradyear','age','gender','friends','cluster')])
teens$cluster=factor(teens$cluster)
# Average age based on each cluster
aggregate(age~cluster,teens,mean)
# Info of a person with its assigned cluster label
(teens[1:10,c('gradyear','age','gender','friends','cluster')])
source('~/R/ML_by_R/Association_Rules.R')
source('~/R/ML_by_R/Spam_SMS.R', encoding = 'UTF-8')
# Creating Training and Testing Data
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- raw[1:4169, ]$v1
sms_test_labels <- raw[4170:5559, ]$v1
prop.table(table(sms_test_labels))
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
spam=subset(raw,v1=='spam')
spam
# Wordcloud of sam and ham
wordcloud(spam$v2,max.words=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham$v2,max.words=40,scale=c(3,0.5))
# Create Dataset of frequent terms
# findFreqTerms takes a dtm  and return a vector of all specified frequent terms
sd=findFreqTerms(sms_dtm,5)
sd[1:10]
length(sd) # 1410
sms_train=sms_dtm_train[,sd]
sms_test=sms_dtm_test[,sd]
sms_train
sms_train[1]
sms_train[1]
sms_train[1,]
# Naive Bayes typically trains on data with categorical values
corp=function(x){ x=ifelse(x>0,'yes','no')}
# apply() can be used on either rows or columns
# MARGIN=1 means row-wise and 2 means column-wise
sms_train=apply(sms_train,MARGIN=2,corp)
sms_test=apply(sms_test,MARGIN = 2,corp)
# Building Model :-
# Default Laplace is also 0
nb=naiveBayes(sms_train,sms_train_labels,laplace=0)
# Evaluating Model:-
pred=predict(nb,sms_test)
# Evaluating Model:-
pred=predict(nb,sms_test,type='prob')
# Evaluating Model:-
pred=predict(nb,sms_test,type='prob')
# Evaluating Model:-
pred=predict(nb,sms_test,type='raw')
pred
# Here we have only 31 out of 1390 misclassified so we have the accuracy of about 97.7%
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
sms_test=sms_dtm_test[,sd]
sms_test=apply(sms_test,MARGIN = 2,corp)
# Evaluating Model:-
pred=predict(nb,sms_test,type='raw')
pred
# Here we have only 31 out of 1390 misclassified so we have the accuracy of about 97.7%
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
length(pred)
length(sms_test_labels)
# Evaluating Model:-
pred=predict(nb,sms_test)
pred
length(pred)
# Here we have only 31 out of 1390 misclassified so we have the accuracy of about 97.7%
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
typeof(pred)
typeof(sms_test_labels)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=1)
pred1=predict(nb,sms_test)
pred1=predict(nb1,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
library(neuralnet)
library(MASS)
# Dataset in MASS function
str(Boston)
data=Boston
hist(data$medv,col='sky blue')
head(data)
# Margin=2
maxvalue=apply(data,2,max)
maxvalue
minvalue=apply(data,2,min)
minvalue
dataf=as.data.frame(scale(data,center=minvalue,scale=maxvalue-minvalue))
summary(dataf)
# It is sameas normalization
norm=function(x){ ((x-min(x))/(max(x)-min(x)))}
dt=as.data.frame(lapply(data,norm))
summary(dt)
ind=sample(1:nrow(dataf),400)
str(ind)
traindf=dataf[ind,]
str(traindf)
testdf=dataf[-ind,]
str(testdf)
allvars=colnames(data)
allvars
predictors=allvars[!allvars %in% 'medv']
predictors
# joining variable with + sign
predictors=paste(predictors,collapse = '+')
predictors
form=as.formula(paste('medv~',predictors,collapse='+'))
form
neuralmodel=neuralnet(formula=form,hidden=c(4,2),linear.output=T,data=traindf)
plot(neuralmodel)
library(kernlab)
letters <- read.csv("letters_data.txt")
str(letters)
train=letters[1:16000,]
test=letters[16001:20000,]
model=ksvm(letter~.,train,kernel='vanilladot',C=1)
model
model
pred=predict(model,test)
head(test$letter)
pred
head(test$letter)
head(pred)
head(pred)
# For checking confusion matrix
v=pred==test$letter
table=table(v)
table
prop.table(table)
# Confusion Matrix
table(pred,test$letter)
prop.table(table)
# Confusion Matrix
table(pred,test$letter)
# improving model by rbf kernel
?ksvm
# C = regularization term
model=ksvm(letter~.,train,kernel='rbfdot',C=1)
pred=predict(model,test)
arg=pred==test$letter
prop.table(table(arg))
library(arules)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.txt',sep=',')
summary(groceries)
groceries
# First 3 purchases
inspect(groceries[1:3])
itemFrequency(groceries[,1:3])
# Frequency on the basis of conditions
itemFrequencyPlot(groceries,support=0.1)
itemFrequencyPlot(groceries,topN=10)
# visualize sparse matrix
image(groceries[1:5])
# sparse matrix of 100 random purchases
image(sample(groceries,100))
# Train model
gro=apriori(groceries,parameter=list(support=0.006,confidence=0.30,minlen=2))
gro
summary(gro)
# Rules and info of first 3 rules
inspect(gro[1:3])
# Sorting of rules by lift indescending order
inspect(sort(gro,by='lift')[1:5])
# Getting rules having product berries
berries=subset(gro,items %in% 'berries')
# Rules with a word fruit anywhere in the Rules like pip fruit, citrus fruit, tropical fruits etc.
fruits=subset(gro,items %pin% 'fruit')
inspect(berries)
inspect(fruits)
# Witing or creating a file consists of found rules
write(gro,'groceries.csv',sep=',',quote=TRUE,row.names=FALSE)
# reading or importing the file consist oof rules in a data frame
groc=read.csv('groceries.csv')
str(groc)
summary(groc)
# converting the rules in data frame format
gr=as(gro,'data.frame')
# The rules column in data frame is converted into factors
str(gr)
library(stats)
teens=read.csv('sns.txt')
str(teens)
# There are NA values in gender and age which means the missing values
summary(teens)
# There are almost 2800 NA values
table(teens$gender,useNA='ifany')
# almost 5000 NA in age
summary(teens$age)
# Teens age can not be too small or too large so, limit it to 13-20 years
teens$age=ifelse(teens$age>13 & teens$age<20,teens$age,NA)
summary(teens$age)
# We can use dummy coding on gender
# We won't create variable for M becoz 0 to Female and no_gender means M
# We need to apply !(is.na(teens$gender)) becoz otherwise NA values will appear in female variables also
teens$female=ifelse(teens$gender=='F' & (!is.na(teens$gender)),1,0)
teens$no_gender=ifelse(is.na(teens$gender),1,0)
table(teens$gender,useNA = 'ifany')
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
# only after extracting NA we can find the mean of a variable
mean(teens$age)
mean(teens$age,na.rm=TRUE)
# Mean age with respect to grad yeR
# aggregate gives the output in a data frame so it is difficult to merge it into our data
aggregate(age~gradyear,teens,mean,na.rm=TRUE)
# We can use ave() to store the information into our dataset
# It gives the vector of the output
# It has 30000 values which are mean of their respective gradyear
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,na.rm=TRUE))
? ave()
ave_age[29995:30000]
summary(ave_age)
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# While we have to find the interest so, exclude the person info 1 to 4 columns
interest=teens[5:40]
# Standardize the data before training:
interest_=as.data.frame(lapply(interest,scale))
# Train the model
set.seed(12345)
model=kmeans(interest_,5)
# cluster centers of each variable
model$centers
# Size or no. of elements in the clusters
model$size
# Cluster label of each data point of the training dataset
model$cluster
# Add cluster label column in the original data set
teens$cluster=model$cluster
# Info of a person with its assigned cluster label
(teens[1:10,c('gradyear','age','gender','friends','cluster')])
teens$cluster=factor(teens$cluster)
# Average age based on each cluster
aggregate(age~cluster,teens,mean)
# Female proportion in each cluster
aggregate(female~cluster,teens,mean)
# Average no. of friends based on each cluster
aggregate(friends~cluster,teens,mean)
levels(teens$cluster) <- c('one',"two", "three",'four','five')
levels(teens$cluster)
teens$cluster
