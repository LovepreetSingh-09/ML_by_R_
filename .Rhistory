table(wdbc$diagnosis)
prop.table(table(wdbc))
prop.table(table(wdbc[1]))
prop.table(table(wdbc[1]))*100
# Use the normalization by creating a function
norm=function(x){(x-min(x))/max(x)-min(x)}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# create Training and Testing data
length(wdbc_norm)
# create Training and Testing data
length(wdbc_norm[1])
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
str(wdbc_norm)
str(wdbc_train)
# Create Training data
wdbc_train=wdbc_norm[1:469,]
wdbc_train_label=wdbc[1:469,1]
str(wdbc_train)
wdbc_norm=as.data.frame(lapply(wdbc[2:32], norm))
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){((x-min(x))/max(x)-min(x))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){(x-min(x))/(max(x)-min(x))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){ return (x-min(x))/(max(x)-min(x))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){
return (x-min(x))/(max(x)-min(x))
}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
return ((x-min(x))/(max(x)-min(x)))
# Use the normalization by creating a function
norm=function(x){
return ((x-min(x))/(max(x)-min(x)))
}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
wdbc_norm=as.data.frame(lapply(wdbc[2:31], normalize))
summary(wdbc_norm)
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){ ((x-min(x))/(max(x)-min(x)))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
wdbc_test_label=wdbc[470:569,]
str(wdbc_train)
# Create Training data
wdbc_train=wdbc_norm[1:469,]
wdbc_train_label=wdbc[1:469,1]
str(wdbc_train)
wdbc_train_label=wdbc[1:469,1]
str(wdbc_train)
# Create Testing data
wdbc_test=wdbc_norm[470:569,]
wdbc_test_label=wdbc[470:569,]
str(wdbc_test_label)
wdbc_test_label=wdbc[470:569,1]
str(wdbc_test_label)
wdbc_test_label
str(wdbc_test)
library(gmodels)
# Training model :-
# Here, k value is the sq. root of the total no. of training samples
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=21)
# Evaluating model performances :-
CrossTable(x=wdbc_test_labels,y=wdbc_test_pred,chisq=FALSE)
# Evaluating model performances :-
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,chisq=FALSE)
# Evaluating model performances :-
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,chisq=TRUE)
# Evaluating model performances :-
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# Improving Model Performance :-
# Now use the standardization where mean is 0
wdbc_z=as.data.frame(scale(wdbc[2:32]))
# Improving Model Performance :-
# Now use the standardization where mean is 0
wdbc_z=as.data.frame(scale(wdbc[2:31]))
str(wdbc_z)
summary(wdbc_z)
# Now again create Traing and Testing data
wdbc_train=wdbc_z[1:469,]
wdbc_test=wdbc_z[470:569,]
str(wdbc_test)
# Build model :-
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=21)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# Now again create Traing and Testing data
wdbc_train=wdbc_z[1:469,]
wdbc_test=wdbc_z[470:569,]
str(wdbc_test)
# Build model :-
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=21)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# The performance didn't chnage but by changng k the the accuracy is different
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=2)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# The performance didn't chnage but by changng k the the accuracy is different
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=15)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# The performance didn't chnage but by changng k the the accuracy is different
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=10)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
corpus=read.csv('spam.csv',sep=',')
str(csv)
str(corpus)
corpus=read.csv('spam.csv',sep=',',headers=TRUE)
corpus=read.csv('spam.csv',sep=',',header=TRUE)
str(corpus)
corpus=read.csv('spam.csv',sep=',')
str(corpus$v1)
prop.table(table(corpus$v1))
str(corpus$v2)
View(corpus)
lenght(corpus$v2)
length(corpus$v2)
length(corpus$x1)
raw=read.csv('spam.csv',sep=',')
length(raw$v2)
str(craw)
prop.table(table(raw$v1))
str(raw)
prop.table(table(raw$v1))
length(raw$X.1)
rwa$x.1[2,]
raw$x.1[2,]
corpus=VCorpus(VectorSource(raw$v2))
install.packages("tm")
VectorSource
prop.table(table(raw$v1))
source('~/.active-rstudio-document', encoding = 'UTF-8')
print(corpus[1:2])
# for displaying text use as.character
as.character(corpus[1])
# For applying it to all use lapply
corpus=lapply(corpus,as.character)
# refers to a volatile corpus—volatile as it is stored in memory as opposed to being stored on disk
# we use the VectorSource() reader function to create a source object from the existing raw$v2 vector.
corpus=VCorpus(VectorSource(raw$v2))
print(corpus[1:2])
# for displaying text use as.character
as.character(corpus[1])
# For applying it to all use lapply
corpus=lapply(corpus,as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
# for displaying text use as.character
as.character(corpus[[1]])
# For applying it to all use lapply
corpus=lapply(corpus,as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
# for displaying text use as.character
as.character(corpus[[1]])
# For applying it to all use lapply
lapply(corpus[1:5],as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
clean_corpus[1:2]
# refers to a volatile corpus—volatile as it is stored in memory as opposed to being stored on disk
# we use the VectorSource() reader function to create a source object from the existing raw$v2 vector.
corpus=VCorpus(VectorSource(raw$v2))
print(corpus[1:2])
# for displaying text use as.character
as.character(corpus[[1]])
# For applying it to all use lapply
lapply(corpus[1:5],as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
clean_corpus[1:2]
as.character(clean_corpus[1:2])
as.character(clean_corpus[[1:2]])
as.character(clean_corpus[[1]])
install.packages("SnowballC")
clean_corpus=tm_map(clean_corpus,removePunctuation)
detach("package:NLP", unload = TRUE)
clean_corpus=tm_map(clean_corpus,removePunctuation)
clean_corpus=tm_map(clean_corpus,stemDocument)
wordStem(c("learn", "learned", "learning", "learns"))
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
as.character(clean_corpus[[1]])
# remove words is a function of tm while stopwords is the collection of most commonly used words
clean_corpus=tm_map(clean_corpus,removeWords,stopwords())
as.character(clean_corpus[[1]])
clean_corpus=tm_map(clean_corpus,removePunctuation)
wordStem(c("learn", "learned", "learning", "learns"))
clean_corpus=tm_map(clean_corpus,stemDocument)
as.character(clean_corpus[[1]])
clean_corpus=tm_map(clean_corpus,stripWhiteSpace)
clean_corpus=tm_map(clean_corpus,stripWhitespace)
as.character(clean_corpus[[1]])
# Tokenization
sm_dtm=DocumentTermMatrix(clean_corpus)
str(sm_dtm)
str(sms_dtm2)
# If we didn't have done the preprocessing then we should prepare data by the following method
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,removeNumbers = TRUE,stopwords = TRUE,removePunctuation = TRUE,stemming = TRUE))
# If we didn't have done the preprocessing then we should prepare data by the following method
sms_dtm2 <- DocumentTermMatrix(corpus, control = list(tolower = TRUE,removeNumbers = TRUE,stopwords = TRUE,removePunctuation = TRUE,stemming = TRUE))
str(sms_dtm2)
sms_dtm
# Tokenization
sms_dtm=DocumentTermMatrix(clean_corpus)
str(sms_dtm)
str(sms_dtm2)
sms_dtm
sms_dtm2
sms_dtm2[1]
sms_dtm[1]
sms_dtm2[1]
sms_dtm[1,]
# Creating Training and Testing Data
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$v1
sms_train_labels <- raw[1:4169, ]$v1
sms_test_labels <- raw[4170:5559, ]$v1
sms_test_labels
table(sms_test_labels)
prop.table(sms_test_labels)
prop.table(factor(sms_test_labels))
prop.table(table(sms_test_labels))
install.packages("wordcloud")
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=70,random.order=FALSE)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
spam=subset(raw$v2,type=='spam')
spam=subset(raw$v2,type=='Spam')
spam=subset(raw,type=='Spam')
spam=subset(raw,type=='spam')
spam=subset(raw,type=='spam')
spam=subset(raw,v1=='spam')
wordcloud(spam,min.freq=40)
wordcloud(spam$v2,min.freq=40)
wordcloud(spam$v2,min.freq=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham,min.freq=40,scale=c(3,0.5))
wordcloud(ham$v2,min.freq=40,scale=c(3,0.5))
wordcloud(spam$v2,max.words=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham$v2,max.words=40,scale=c(3,0.5))
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
source('~/R/ML_by_R/Spam_SMS.R', encoding = 'UTF-8')
# Tokenization
sms_dtm=DocumentTermMatrix(clean_corpus)
str(sms_dtm)
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
# Create Dataset of frequent terms
# findFreqTerms takes a dtm  and return a vector of all specified frequent terms
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
sms_train=sms_dtm_train[,sd]
sms_test=sms_dtm_test[,sd]
# Naive Bayes typically trains on data with categorical values
corp=function(x){ x=ifelse(x>0,'yes','no')}
# apply() can be used on either rows or columns
# MARGIN=1 means row-wise and 2 means column-wise
sms_train=apply(sms_train,MARGIN=2,corp)
sms_test=apply(sms_test,MARGIN = 2,corp)
install.packages("e1071")
library(e1071)
library(gmodels)
# Building Model :-
# Default Laplace is also 0
nb=naiveBayes(sms_train,sms_train_labels,laplace=0)
# Evaluating Model:-
pred=predict(nb,sms_test)
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'))
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=1)
pred1=predict(nb,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=3)
pred1=predict(nb,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
sd[1:10]
length(sd)
install.packages("C50")
library(C50)
credit=read.csv('credit.data')
str(credit)
credit=read.csv('credit.data',header=TRUE)
str(credit)
credit=read.csv('credit.txt',header=TRUE)
str(credit)
x=seq(0,1,by=0.01)
x
curve(x,-x*log2(x)-(1-x)*log2(1-x))
type(c)
type(x)
typeof(x)
typeof(x)
curve(x,-x*log2(x)-(1-x)*log2(1-x))
x=c(seq(0,1,by=0.01))
typeof(x)
v=-x*log2(x)-(1-x)*log2(1-x)
print(v)
curve(x,v)
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy", lwd = 4)
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy")
rm()
rm(list=ls())
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=3,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=2.7,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1.5,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1.5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
library(C50)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
library(C50)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
set.seed(1)
sd=sample(1000,900)
credit$default
credit_train_labels=credit$default[,sd]
credit_train_labels=credit$default[sd,]
credit_train_labels=credit$default[sd]
credit_train=credit[sd,-credit$default]
str(crdit_train)
str(credit_train)
credit_train$default
col[credit_train]
str(credit)
str(credit)
str(credit_train)
credit_train=credit[sd,-credit$default]
str(credit_train)
credit_train=credit[sd,]
credit_train_labels=credit$default[sd]
str(credit_train)
credit_train=credit[sd,-17]
credit_train_labels=credit$default[sd]
str(credit_train)
credit_test=credit[-sd,-17]
str(credit_test)
credit_train=credit[sd,-'default']
credit_train=credit[sd,-c('default')]
credit_train=credit[sd,c(-'default')]
credit_train=credit[sd,-17]
credit_test_labels=credit[-sd,17]
table(credit_test_labels)
table(credit_test_labels)
table(credit_train_labels)
# trails are the number of trees to be made for boosting
# Costs is a matrix associated with the different types of errors.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
table(credit_train_labels)
str(credit_train_labels)
credit_train_labels=factor(credit$default[sd])
# trails are the number of trees to be made for boosting. 1 by default.
# Costs is a matrix associated with the different types of errors. NULL by default.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
summary(model)
# Evaluating the model
pred=predict.C5.0(model,credit_test)
library(gmodels)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Summary shows the rules by which tree is made of.
model
summary(model)
# Evaluating the model
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
crop=function(x){x=ifelse(x>1,'Yes','No')}
credit_train_labels=apply(factor(credit$default[sd]),corp)
credit_train_labels=lapply(factor(credit$default[sd]),corp)
credit_train_labels=lapply(factor(credit$default[sd]),crop)
credit_train_labels=apply(factor(credit$default[sd]),crop)
credit_train_labels=sapply(factor(credit$default[sd]),crop)
credit_train_labels=sapply((credit$default[sd]),crop)
table(credit_train_labels)
credit_train_labels=apply((credit$default[sd]),crop)
credit_train_labels=lapply((credit$default[sd]),crop)
str(credit_train)
table(credit_train_labels)
table(credit_train_labels)
credit_train_labels=sapply((credit$default[sd]),crop)
table(credit_train_labels)
credit_train_labels=factor(sapply((credit$default[sd]),crop))
table(credit_train_labels)
str(credit_train_labels)
credit_test_labels=factor(sapply(credit[-sd,17],crop))
table(credit_test_labels)
source('~/R/ML_by_R/Loan_model.R')
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=10)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=15)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=5)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=3)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=4)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Using Cost Function
matrix=list(c('No','Yes'),c('No','Yes'))
matrix
names(matrix)=c('actual','predicted')
matrix
costf=matrix(c(0,1,4,0),nrows=2,dimnames=matrix)
costf=matrix(c(0,1,4,0),nrow=2,dimnames=matrix)
costf
costf=matrix(c(0,4,1,0),nrow=2,dimnames=matrix)
costf
model=C5.0(credit_train,credit_train_labels,trials=4,costs=costf)
model
summary(model)
model=C5.0(credit_train,credit_train_labels,trials=10,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=4,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=15,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=11,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=10,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=12,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
