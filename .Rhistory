library(gmodels)
library(irr)
library(rpart)
library(rpart.plot)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Generating Random Numbers
set.seed(1)
sd=sample(1000,900)
crop=function(x){x=ifelse(x>1,'Yes','No')}
# spliting data for training and testing
credit_train=credit[sd,-17]
credit_train_labels=factor(sapply((credit$default[sd]),crop))
str(credit_train)
str(credit_train_labels)
table(credit_train_labels)
credit_test=credit[-sd,-17]
str(credit_test)
credit_test_labels=factor(sapply(credit[-sd,17],crop))
table(credit_test_labels)
credit$default=factor(sapply((credit$default),crop))
str(credit)
# trails are the number of trees to be made for boosting. 1 by default.
# Costs is a matrix associated with the different types of errors. NULL by default.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
# model gives the depth or size, samples and features used
# Summary shows the rules by which tree is made of and the error rate.
model
summary(model)
rpart.plot(model)
# Evaluating the model
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=4)
model
summary(model)
pred=predict(model,credit_test,type='prob')
pred
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Using Cost Function
# Costs is used to reduce the no. of False Negative or Positive at the expense of the other
matrix=list(c('No','Yes'),c('No','Yes'))
names(matrix)=c('actual','predicted')
matrix
costf=matrix(c(0,4,1,0),nrow=2,dimnames=matrix)
costf
model=C5.0(credit_train,credit_train_labels,trials=12,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
# Accuracy has been got better slightly but the False negative and False Positive have been changed.
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Creating random no.
random_ids=order(runif(1000))
random_ids
credit_train <- credit[random_ids[1:500], ]
credit_validate <- credit[random_ids[501:750], ]
credit_test <- credit[random_ids[751:1000], ]
# Using Stratified Random Sampling method
# The classes of default column will be partitioned on the proportion p=0.75 and will not returns list
it=createDataPartition(credit$default,p=0.75,list=FALSE)
length(it) # 750
credit_train=credit[it,]
credit_test=credit[-it,]
# using KFold Cross-Validation
set.seed(123)
folds=createFolds(credit$default,k=10)
# 100 samples in each fold with equal portion of default labels
# Every fold samples are unique
# Every fold samples will be used in test dataset while remaining samples will be for training 10 times
str(folds)
credit$default=factor(credit$default)
cv_results=lapply(folds,function(x) {
credit_train=credit[-x,]
credit_test=credit[x,]
credit_model=C5.0(default~.,data=credit_train)
credit_pred=predict(credit_model,credit_train)
credit_actual=credit_test$default
# kaapa2 from irr package
kp=kappa2(data.frame(credit_actual,credit_pred))$value
return (kp)
})
# cv_result is list
str(cv_results)
# use unlist function to unlist
mean(unlist(cv_results))
modelLookup('C5.0')
?expand.grid
# Probability of yes or no
head(predict(m,credit,'prob'))
table(p, credit$default)
m <- train(default ~ ., data = credit, method = "C5.0")
p <- predict(m, credit)
# Rule Learner Algorithms
library(RWeka)
mushroom=read.csv('mushrooms.csv')
str(mushroom)
# As veil.type has only one value
mushroom$veil.type<-NULL
table(mushroom$class)
# Training a 1R model from RWeka
# Here class represent the target variable while tilde refers to the features to use.
# Here . after ~ means all the features ut if we want to specify some certtain features, we need to put + b/w those
# like... OneR(class~odor+bruises,mushroom)
# Mushroom is a dataframe
model=OneR(class~.,mushroom)
library(psych)
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
str(insurance)
str(challanger)
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
# Exploring relationship b/w all variables by pearson correlation
# The diagonal here will be 1 becoz of variables covariance with itself and its variance is same
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
# you can write . in place of all independent variables
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
model
summary(model) # all estimates, p-value and R2 value
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
library(caTools)
library(ggplot2)
library(gmodels)
getwd()
dataset=read.csv('Salary_Data.csv')
str(dataset)
split=sample.split(dataset$Salary,SplitRatio=0.7)
# returns the split in True/False values
split
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
# Build model
model=lm(formula=Salary~YearsExperience,data=train_dataset)
model # shows intercept and weight value
summary(model)
coef(model)
ypred=predict(model,newdata=test_data)
ggplot()+geom_point(aes(x=train_dataset$YearsExperience,y=train_dataset$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience),y=predict(model,newdata=train_dataset),color='blue')+
ggtitle('Salary vs Experience')+
xlab('Years of Experience')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
labs(x='Years',y='salary')
library(neuralnet)
library(MASS)
# Dataset in MASS function
str(Boston)
data=Boston
hist(data$medv,col='sky blue')
head(data)
# Margin=2
maxvalue=apply(data,2,max)
maxvalue
minvalue=apply(data,2,min)
minvalue
dataf=as.data.frame(scale(data,center=minvalue,scale=maxvalue-minvalue))
summary(dataf)
# It is sameas normalization
norm=function(x){ ((x-min(x))/(max(x)-min(x)))}
dt=as.data.frame(lapply(data,norm))
summary(dt)
ind=sample(1:nrow(dataf),400)
str(ind)
traindf=dataf[ind,]
str(traindf)
testdf=dataf[-ind,]
str(testdf)
allvars=colnames(data)
allvars
predictors=allvars[!allvars %in% 'medv']
predictors
# joining variable with + sign
predictors=paste(predictors,collapse = '+')
predictors
form=as.formula(paste('medv~',predictors,collapse='+'))
form
neuralmodel=neuralnet(formula=form,hidden=c(4,2),linear.output=T,data=traindf)
plot(neuralmodel)
library(rpart)
library(rpart.plot)
library(RWeka)
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(at2))
sdr1
sdr2=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
sdr2
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
# Dataset in MASS function
str(Boston)
hist(data$quality,col='sky blue')
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
model=rpart(quality~.,w_train)
model
# The nodes having * can be used for predictions
summary(model)
model
# The nodes having * can be used for predictions
summary(model)
# Visualizing Tree
rpart.plot(model,digits=4,type=3,fallen.leaves=TRUE,extra = 101)
pred=predict(model,w_test)
summary(pred)
summary(w_test$quality)
cor(pred,w_test$quality)
# So we need to check mean absolute error
MAE=function(act,pred){
mean(abs(act-pred))
}
MAE(w_test$quality,pred)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
data=read.csv('concrete.txt')
str(data)
# Replacing NA values by 0
data[is.na(data)]=0
summary(data)
# Exclude missing values
data=na.omit(data)
str(data)
summary(data)
# Normalize the data
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(data,norm))
summary(concrete)
set.seed(12345)
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
length(sp)
train=concrete[sp,]
test=concrete[-sp,]
str(test)
# by default, hidden =1 indicates 1 hidden node
model=neuralnet(strength~.,train,hidden=c(2))
model # Shows each weight and intercept/bias
# from plotting we see that SSE is 4.011 while steps(iterations) are 851
plot(model)
# Compute returns a list with two components: $neurons, which stores the neurons for each layer in the network
# and $net.result, which stores the predicted values.
pred=compute(model,test)
pred$neurons
pred$net.result
cor(pred$net.result,test$strength) # 0.821
# Make neural network with 4 hidden nodes
model=neuralnet(strength~.,train,hidden=4)
model
plot(model) # Now our SSE error is reduced to approx. 1 and steps has been increased
pred=compute(model,test)
cor(pred$net.result,test$strength)
library(kernlab)
letters <- read.csv("letters_data.txt")
str(letters)
train=letters[1:16000,]
test=letters[16001:20000,]
model=ksvm(letter~.,train,kernel='vanilladot',C=1)
model
pred=predict(model,test)
head(test$letter)
head(pred)
# For checking confusion matrix
v=pred==test$letter
table=table(v)
table
prop.table(table)
# Confusion Matrix
table(pred,test$letter)
# improving model by rbf kernel
?ksvm
# C = regularization term
model=ksvm(letter~.,train,kernel='rbfdot',C=1)
pred=predict(model,test)
arg=pred==test$letter
prop.table(table(arg))
library(arules)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.txt',sep=',')
summary(groceries)
fix(groceries)
summary(groceries)
groceries
# First 3 purchases
inspect(groceries[1:3])
# First 3 purchases
inspect(groceries[1:3])
itemFrequency(groceries[,1:3])
# Frequency on the basis of conditions
itemFrequencyPlot(groceries,support=0.1)
itemFrequencyPlot(groceries,topN=10)
# visualize sparse matrix
image(groceries[1:5])
# sparse matrix of 100 random purchases
image(sample(groceries,100))
# Train model
gro=apriori(groceries,parameter=list(support=0.006,confidence=0.30,minlen=2))
gro
summary(gro)
# Rules and info of first 3 rules
inspect(gro[1:3])
# Sorting of rules by lift indescending order
inspect(sort(gro,by='lift')[1:5])
# Getting rules having product berries
berries=subset(gro,items %in% 'berries')
# Rules with a word fruit anywhere in the Rules like pip fruit, citrus fruit, tropical fruits etc.
fruits=subset(gro,items %pin% 'fruit')
inspect(berries)
library(arules)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.txt',sep=',')
View(folds)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.txt')
fix(groceries)
summary(groceries)
groceries
# First 3 purchases
inspect(groceries[1:3])
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.txt',sep=',')
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.txt',sep=',')
fix(groceries)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.csv',sep=',')
View(gro)
summary(groceries)
groceries
# First 3 purchases
inspect(groceries[1:3])
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries.csv',sep=',')
fix(groceries)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries_data.csv',sep=',')
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries_data.txt',sep=',')
fix(groceries)
# read.transactions creates a sparse matrix to save memory and computational efficiency
groceries=read.transactions('groceries_data.txt',sep=',')
fix(groceries)
summary(gro)
# Rules and info of first 3 rules
inspect(gro[1:3])
# Sorting of rules by lift indescending order
inspect(sort(gro,by='lift')[1:5])
# Getting rules having product berries
berries=subset(gro,items %in% 'berries')
# Rules with a word fruit anywhere in the Rules like pip fruit, citrus fruit, tropical fruits etc.
fruits=subset(gro,items %pin% 'fruit')
inspect(berries)
inspect(fruits)
# reading or importing the file consist oof rules in a data frame
groc=read.csv('groceries.csv')
str(groc)
summary(groc)
# converting the rules in data frame format
gr=as(gro,'data.frame')
# The rules column in data frame is converted into factors
str(gr)
library(stats)
teens=read.csv('sns.txt')
str(teens)
# There are NA values in gender and age which means the missing values
summary(teens)
# There are almost 2800 NA values
table(teens$gender,useNA='ifany')
# almost 5000 NA in age
summary(teens$age)
# Teens age can not be too small or too large so, limit it to 13-20 years
teens$age=ifelse(teens$age>13 & teens$age<20,teens$age,NA)
summary(teens$age)
# We can use dummy coding on gender
# We won't create variable for M becoz 0 to Female and no_gender means M
# We need to apply !(is.na(teens$gender)) becoz otherwise NA values will appear in female variables also
teens$female=ifelse(teens$gender=='F' & (!is.na(teens$gender)),1,0)
teens$no_gender=ifelse(is.na(teens$gender),1,0)
table(teens$gender,useNA = 'ifany')
table(teens$female,useNA='ifany')
table(teens$no_gender,useNA='ifany')
# only after extracting NA we can find the mean of a variable
mean(teens$age)
mean(teens$age,na.rm=TRUE)
# Mean age with respect to grad yeR
# aggregate gives the output in a data frame so it is difficult to merge it into our data
aggregate(age~gradyear,teens,mean,na.rm=TRUE)
# We can use ave() to store the information into our dataset
# It gives the vector of the output
# It has 30000 values which are mean of their respective gradyear
ave_age=ave(teens$age,teens$gradyear,FUN=function(x) mean(x,na.rm=TRUE))
? ave()
ave_age[29995:30000]
summary(ave_age)
teens$age=ifelse(is.na(teens$age),ave_age,teens$age)
summary(teens$age)
# While we have to find the interest so, exclude the person info 1 to 4 columns
interest=teens[5:40]
# Standardize the data before training:
interest_=as.data.frame(lapply(interest,scale))
# Train the model
set.seed(12345)
model=kmeans(interest_,5)
# cluster centers of each variable
model$centers
# Size or no. of elements in the clusters
model$size
# Cluster label of each data point of the training dataset
model$cluster
# Add cluster label column in the original data set
teens$cluster=model$cluster
# Info of a person with its assigned cluster label
(teens[1:10,c('gradyear','age','gender','friends','cluster')])
teens$cluster=factor(teens$cluster)
# Average age based on each cluster
aggregate(age~cluster,teens,mean)
# Female proportion in each cluster
aggregate(female~cluster,teens,mean)
# Average no. of friends based on each cluster
aggregate(friends~cluster,teens,mean)
levels(teens$cluster) <- c('one',"two", "three",'four','five')
levels(teens$cluster)
teens$cluster
library(rpart)
library(rpart.plot)
iris
s=sample(150,100)
s
str(s)
iris_train=iris[s,]
iris_test=iris[-s,]
iris_train
iris_test
dtm=rpart(Species~.,iris_train,method='class')
dtm
plot(dtm)
text(dtm)
plot(dtm)
rpart.plot(dtm)
rpart.plot(dtm,type=4,extra=101)
p=predict(dtm,iris_test,type='class')
str(iris)
p
iris_test[,5]
table(iris_test[,5],p)
model=C5.0(Species~.,iris_train,method='class')
?C5.0
model
str(model)
predict(model,iris_test)
table(credit_pred, credit$default)
