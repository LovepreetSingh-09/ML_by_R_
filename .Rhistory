credit_train_labels=sapply((credit$default[sd]),crop)
table(credit_train_labels)
credit_train_labels=apply((credit$default[sd]),crop)
credit_train_labels=lapply((credit$default[sd]),crop)
str(credit_train)
table(credit_train_labels)
table(credit_train_labels)
credit_train_labels=sapply((credit$default[sd]),crop)
table(credit_train_labels)
credit_train_labels=factor(sapply((credit$default[sd]),crop))
table(credit_train_labels)
str(credit_train_labels)
credit_test_labels=factor(sapply(credit[-sd,17],crop))
table(credit_test_labels)
source('~/R/ML_by_R/Loan_model.R')
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=10)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=15)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=5)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=3)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=4)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Using Cost Function
matrix=list(c('No','Yes'),c('No','Yes'))
matrix
names(matrix)=c('actual','predicted')
matrix
costf=matrix(c(0,1,4,0),nrows=2,dimnames=matrix)
costf=matrix(c(0,1,4,0),nrow=2,dimnames=matrix)
costf
costf=matrix(c(0,4,1,0),nrow=2,dimnames=matrix)
costf
model=C5.0(credit_train,credit_train_labels,trials=4,costs=costf)
model
summary(model)
model=C5.0(credit_train,credit_train_labels,trials=10,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=4,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=15,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=11,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=10,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=12,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
library(RWeka)
mushroom=read.csv('mushroom.data')
str(mushroom)
mushroom=read.table('mushroom.data')
str(mushroom)
mushroom=read.csv('mushroom.data')
str(mushroom)
mushroom$p.2
mushroom=read.csv('mushrooms.csv')
str(mushroom)
mushroom$veil.type
mushroom$veil.type<-NULL
summary(mushroom)
table(mushroom$class)
# Training a 1R model from RWeka
model=OneR(class~.,mushroom)
model
summary(model)
# Improving Performance :-
# Use JRip() another rule algorithm from RWeka
model=JRip(class~.,mushroom)
model
# It has more rules than the previous one
summary(model)
install.packages("tidyverse")
challanger=read.csv('challanger.txt')
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
challanger=read.csv('challanger.txt')
str(insurance)
str(challanger)
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
str(insurance)
str(challanger)
# wight by covariance and variance matrix formula
b=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
b
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)='estimate'
print(b)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
print(w)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)='estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
column(w)<-'estimate'
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = launch$distress_ct, x = launch[2:4])
reg(y = challanger$distress_ct, x = challanger[2:4])
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
# Predicting Medical Expenses
summary(insurance$expenses)
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
# Exploring relationship b/w all variables by pearson correlation
cor(insurance)
# Exploring relationship b/w all variables by pearson correlation
cor(insurance[c("age", "bmi", "children", "expenses")])
# Exploring relationship b/w all variables by pearson correlation
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
library(psych)
install.packages("psych")
library(psych)
# scatter plot using psych
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
summary(model)
model
summary(model)
# Convert bmi into binary
insurance$bmi=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi,data=insurance)
model
summary(model)
# Convert bmi into binary
insurance$bmi=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi,data=insurance)
model
summary(model)
insurance=read.csv('insurance.txt')
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
insurance=read.csv('insurance.txt')
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
summary(model)
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
split=sample.split(30,SplitRatio=0.7)
dataset$Salary
getwd()
dataset=read.csv('Salary_Data.csv')
str(dataset)
split=sample.split(dataset$Salary,SplitRatio=0.7)
library(caTools)
split=sample.split(dataset$Salary,SplitRatio=0.7)
split=sample.split(30,SplitRatio=0.7)
# returns the split in True/False values
split
split=sample.split(1:30,SplitRatio=0.7)
# returns the split in True/False values
split
summary(model)
ypred=predict(model,newdata=test_data)
summary(ypred)
ypred=predict(model,newdata=test_data)
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
# Build model
model=lm(formula=Salary~YearsExperience,data=train_dataset)
model # shows intercept and weight value
summary(model)
coef(model)
ypred=predict(model,newdata=test_data)
summary(ypred)
source('~/R/ML_by_R/Medicine_Expenses.R')
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
ggplot()+geom_point(aes(x=train_dataset$YearsExperience,y=train_dataset$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience),y=predict(model,newdata=train_dataset),color='blue')+
ggtitle('Salary vs Experience')+
xlab('Years of Experience')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
library(caTools)
library(ggplot2)
library(gmodels)
library(ggplot2)
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
source('~/R/ML_by_R/Salary_Linear_Regression.R')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
insurance=read.csv('insurance.txt')
str(insurance)
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
View(insurance)
# Exploring relationship b/w all variables by pearson correlation
# The diagonal here will be 1 becoz of variables covariance with itself and its variance is same
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
# you can write . in place of all independent variables
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
---
title: "R Notebook"
output: html_notebook
---
sdr1
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(tee))
sdr1
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(bt1))
sdr1
sdr1=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(bt1))
sdr1
sdr2=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
sdr2
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(at2))
sdr1
sdr2=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
sdr2
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('whitewines.txt')
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
hist(data$quality)
hist(data$quality,col='light blue')
hist(data$quality,col='sky blue')
install.packages("rpart")
install.packages("rpart.plot")
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
model
model=rpart(quality~.,w_train)
library(rpart)
model=rpart(quality~.,w_train)
model
# The nodes having * can be used for predictions
summary(model)
rpart.plot(model)
library(rpart.plot)
rpart.plot(model)
rpart.plot(model,digits=4,type=3,fallen.leaves=TRUE,extra = 101)
pred=predict(model,w_test)
summary(pred)
summary(data$quality)
cor(pred,data$quality)
cor(pred,data$quality)
cor(pred,w_test$quality)
summary(w_test$quality)
cor(pred,w_test$quality)
# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))}
# So we need to check mean absolute error
MAE=function(act,pred){return mean(abs(act-pred))}
# So we need to check mean absolute error
MAE<-function(act,pred){
return mean(abs(act-pred))
}
MAE <- function(actual, predicted) {
mean(abs(actual - predicted))
}
# So we need to check mean absolute error
MAE<-function(act,pred){
mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
mean(abs(act-pred))
}
MAE(w_test$quality,pred)
library(RWeka)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
hist(data$quality,col='sky blue')
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
model=MSP(quality~.,data=w_train)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
m.m5p=M5P(quality~.,data=w_train)
summary(m.m5p)
m.m5p
pred=predict(m.m5p,w_test)
m.m5p <- M5P(quality ~ ., data = w_train)
m.m5p
summary(m.m5p)
