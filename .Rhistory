str(sm_dtm)
str(sms_dtm2)
# If we didn't have done the preprocessing then we should prepare data by the following method
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,removeNumbers = TRUE,stopwords = TRUE,removePunctuation = TRUE,stemming = TRUE))
# If we didn't have done the preprocessing then we should prepare data by the following method
sms_dtm2 <- DocumentTermMatrix(corpus, control = list(tolower = TRUE,removeNumbers = TRUE,stopwords = TRUE,removePunctuation = TRUE,stemming = TRUE))
str(sms_dtm2)
sms_dtm
# Tokenization
sms_dtm=DocumentTermMatrix(clean_corpus)
str(sms_dtm)
str(sms_dtm2)
sms_dtm
sms_dtm2
sms_dtm2[1]
sms_dtm[1]
sms_dtm2[1]
sms_dtm[1,]
# Creating Training and Testing Data
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$v1
sms_train_labels <- raw[1:4169, ]$v1
sms_test_labels <- raw[4170:5559, ]$v1
sms_test_labels
table(sms_test_labels)
prop.table(sms_test_labels)
prop.table(factor(sms_test_labels))
prop.table(table(sms_test_labels))
install.packages("wordcloud")
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=70,random.order=FALSE)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
spam=subset(raw$v2,type=='spam')
spam=subset(raw$v2,type=='Spam')
spam=subset(raw,type=='Spam')
spam=subset(raw,type=='spam')
spam=subset(raw,type=='spam')
spam=subset(raw,v1=='spam')
wordcloud(spam,min.freq=40)
wordcloud(spam$v2,min.freq=40)
wordcloud(spam$v2,min.freq=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham,min.freq=40,scale=c(3,0.5))
wordcloud(ham$v2,min.freq=40,scale=c(3,0.5))
wordcloud(spam$v2,max.words=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham$v2,max.words=40,scale=c(3,0.5))
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
source('~/R/ML_by_R/Spam_SMS.R', encoding = 'UTF-8')
# Tokenization
sms_dtm=DocumentTermMatrix(clean_corpus)
str(sms_dtm)
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
# Create Dataset of frequent terms
# findFreqTerms takes a dtm  and return a vector of all specified frequent terms
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
sms_train=sms_dtm_train[,sd]
sms_test=sms_dtm_test[,sd]
# Naive Bayes typically trains on data with categorical values
corp=function(x){ x=ifelse(x>0,'yes','no')}
# apply() can be used on either rows or columns
# MARGIN=1 means row-wise and 2 means column-wise
sms_train=apply(sms_train,MARGIN=2,corp)
sms_test=apply(sms_test,MARGIN = 2,corp)
install.packages("e1071")
library(e1071)
library(gmodels)
# Building Model :-
# Default Laplace is also 0
nb=naiveBayes(sms_train,sms_train_labels,laplace=0)
# Evaluating Model:-
pred=predict(nb,sms_test)
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'))
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=1)
pred1=predict(nb,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=3)
pred1=predict(nb,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
sd[1:10]
length(sd)
install.packages("C50")
library(C50)
credit=read.csv('credit.data')
str(credit)
credit=read.csv('credit.data',header=TRUE)
str(credit)
credit=read.csv('credit.txt',header=TRUE)
str(credit)
x=seq(0,1,by=0.01)
x
curve(x,-x*log2(x)-(1-x)*log2(1-x))
type(c)
type(x)
typeof(x)
typeof(x)
curve(x,-x*log2(x)-(1-x)*log2(1-x))
x=c(seq(0,1,by=0.01))
typeof(x)
v=-x*log2(x)-(1-x)*log2(1-x)
print(v)
curve(x,v)
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy", lwd = 4)
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy")
rm()
rm(list=ls())
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=3,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=2.7,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1.5,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1.5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
library(C50)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
library(C50)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
set.seed(1)
sd=sample(1000,900)
credit$default
credit_train_labels=credit$default[,sd]
credit_train_labels=credit$default[sd,]
credit_train_labels=credit$default[sd]
credit_train=credit[sd,-credit$default]
str(crdit_train)
str(credit_train)
credit_train$default
col[credit_train]
str(credit)
str(credit)
str(credit_train)
credit_train=credit[sd,-credit$default]
str(credit_train)
credit_train=credit[sd,]
credit_train_labels=credit$default[sd]
str(credit_train)
credit_train=credit[sd,-17]
credit_train_labels=credit$default[sd]
str(credit_train)
credit_test=credit[-sd,-17]
str(credit_test)
credit_train=credit[sd,-'default']
credit_train=credit[sd,-c('default')]
credit_train=credit[sd,c(-'default')]
credit_train=credit[sd,-17]
credit_test_labels=credit[-sd,17]
table(credit_test_labels)
table(credit_test_labels)
table(credit_train_labels)
# trails are the number of trees to be made for boosting
# Costs is a matrix associated with the different types of errors.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
table(credit_train_labels)
str(credit_train_labels)
credit_train_labels=factor(credit$default[sd])
# trails are the number of trees to be made for boosting. 1 by default.
# Costs is a matrix associated with the different types of errors. NULL by default.
model=C5.0(credit_train,credit_train_labels,trials=1,costs=NULL)
summary(model)
# Evaluating the model
pred=predict.C5.0(model,credit_test)
library(gmodels)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Summary shows the rules by which tree is made of.
model
summary(model)
# Evaluating the model
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
crop=function(x){x=ifelse(x>1,'Yes','No')}
credit_train_labels=apply(factor(credit$default[sd]),corp)
credit_train_labels=lapply(factor(credit$default[sd]),corp)
credit_train_labels=lapply(factor(credit$default[sd]),crop)
credit_train_labels=apply(factor(credit$default[sd]),crop)
credit_train_labels=sapply(factor(credit$default[sd]),crop)
credit_train_labels=sapply((credit$default[sd]),crop)
table(credit_train_labels)
credit_train_labels=apply((credit$default[sd]),crop)
credit_train_labels=lapply((credit$default[sd]),crop)
str(credit_train)
table(credit_train_labels)
table(credit_train_labels)
credit_train_labels=sapply((credit$default[sd]),crop)
table(credit_train_labels)
credit_train_labels=factor(sapply((credit$default[sd]),crop))
table(credit_train_labels)
str(credit_train_labels)
credit_test_labels=factor(sapply(credit[-sd,17],crop))
table(credit_test_labels)
source('~/R/ML_by_R/Loan_model.R')
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=10)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=15)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=5)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=3)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Improving Model Performance :-
# Firstly increase the no. of trials
model=C5.0(credit_train,credit_train_labels,trials=4)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
# Using Cost Function
matrix=list(c('No','Yes'),c('No','Yes'))
matrix
names(matrix)=c('actual','predicted')
matrix
costf=matrix(c(0,1,4,0),nrows=2,dimnames=matrix)
costf=matrix(c(0,1,4,0),nrow=2,dimnames=matrix)
costf
costf=matrix(c(0,4,1,0),nrow=2,dimnames=matrix)
costf
model=C5.0(credit_train,credit_train_labels,trials=4,costs=costf)
model
summary(model)
model=C5.0(credit_train,credit_train_labels,trials=10,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=4,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=15,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=11,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=10,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
model=C5.0(credit_train,credit_train_labels,trials=12,costs=costf)
model
summary(model)
pred=predict(model,credit_test)
CrossTable(credit_test_labels,pred,prop.chissq=F,prop.t=FALSE,dnn=c('Actual','Prediction'))
library(RWeka)
mushroom=read.csv('mushroom.data')
str(mushroom)
mushroom=read.table('mushroom.data')
str(mushroom)
mushroom=read.csv('mushroom.data')
str(mushroom)
mushroom$p.2
mushroom=read.csv('mushrooms.csv')
str(mushroom)
mushroom$veil.type
mushroom$veil.type<-NULL
summary(mushroom)
table(mushroom$class)
# Training a 1R model from RWeka
model=OneR(class~.,mushroom)
model
summary(model)
# Improving Performance :-
# Use JRip() another rule algorithm from RWeka
model=JRip(class~.,mushroom)
model
# It has more rules than the previous one
summary(model)
install.packages("tidyverse")
challanger=read.csv('challanger.txt')
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
challanger=read.csv('challanger.txt')
str(insurance)
str(challanger)
challanger=read.csv('challanger.txt')
insurance=read.csv('insurance.txt')
str(insurance)
str(challanger)
# wight by covariance and variance matrix formula
b=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
b
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)='estimate'
print(b)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
print(w)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)='estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
column(w)<-'estimate'
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = launch$distress_ct, x = launch[2:4])
reg(y = challanger$distress_ct, x = challanger[2:4])
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
# Predicting Medical Expenses
summary(insurance$expenses)
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
# Exploring relationship b/w all variables by pearson correlation
cor(insurance)
# Exploring relationship b/w all variables by pearson correlation
cor(insurance[c("age", "bmi", "children", "expenses")])
# Exploring relationship b/w all variables by pearson correlation
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
library(psych)
install.packages("psych")
library(psych)
# scatter plot using psych
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
summary(model)
model
summary(model)
# Convert bmi into binary
insurance$bmi=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi,data=insurance)
model
summary(model)
# Convert bmi into binary
insurance$bmi=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi,data=insurance)
model
summary(model)
insurance=read.csv('insurance.txt')
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
insurance=read.csv('insurance.txt')
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
summary(model)
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
split=sample.split(30,SplitRatio=0.7)
dataset$Salary
getwd()
dataset=read.csv('Salary_Data.csv')
str(dataset)
split=sample.split(dataset$Salary,SplitRatio=0.7)
library(caTools)
split=sample.split(dataset$Salary,SplitRatio=0.7)
split=sample.split(30,SplitRatio=0.7)
# returns the split in True/False values
split
split=sample.split(1:30,SplitRatio=0.7)
# returns the split in True/False values
split
summary(model)
ypred=predict(model,newdata=test_data)
summary(ypred)
ypred=predict(model,newdata=test_data)
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
# Build model
model=lm(formula=Salary~YearsExperience,data=train_dataset)
model # shows intercept and weight value
summary(model)
coef(model)
ypred=predict(model,newdata=test_data)
summary(ypred)
