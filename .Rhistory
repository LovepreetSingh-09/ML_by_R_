b=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
b
# wight by covariance and variance matrix formula
m=cov(challanger$temperature,challanger$distress_ct)/var(challanger$temperature)
m
# intercept
b=mean(challanger$distress_ct)-m*(mean(challanger$temperature))
b
cor(challanger$temperature,challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)='estimate'
print(b)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
print(w)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)='estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
column(w)<-'estimate'
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
column(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,x)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = launch$distress_ct, x = launch[2:4])
reg(y = challanger$distress_ct, x = challanger[2:4])
# function for getting both intercept and weights
reg=function(x,y){
a=as.matrix(x)
# adding a new column with all values=1
a=cbind(Intercept=1,a)
# %*% is for matrix multiplication
w=solve(t(a)%*%a)%*%t(a)%*%y
colnames(w)<-'estimate'
print(w)
}
reg(x=challanger$temperature,y=challanger$distress_ct)
reg(y = challanger$distress_ct, x = challanger[2:4])
# Predicting Medical Expenses
summary(insurance$expenses)
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
# Exploring relationship b/w all variables by pearson correlation
cor(insurance)
# Exploring relationship b/w all variables by pearson correlation
cor(insurance[c("age", "bmi", "children", "expenses")])
# Exploring relationship b/w all variables by pearson correlation
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
library(psych)
install.packages("psych")
library(psych)
# scatter plot using psych
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
summary(model)
model
summary(model)
# Convert bmi into binary
insurance$bmi=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi,data=insurance)
model
summary(model)
# Convert bmi into binary
insurance$bmi=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi,data=insurance)
model
summary(model)
insurance=read.csv('insurance.txt')
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>30,1,0)
insurance$age=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
insurance=read.csv('insurance.txt')
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + age*bmi2,data=insurance)
model
summary(model)
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
summary(model)
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
split=sample.split(30,SplitRatio=0.7)
dataset$Salary
getwd()
dataset=read.csv('Salary_Data.csv')
str(dataset)
split=sample.split(dataset$Salary,SplitRatio=0.7)
library(caTools)
split=sample.split(dataset$Salary,SplitRatio=0.7)
split=sample.split(30,SplitRatio=0.7)
# returns the split in True/False values
split
split=sample.split(1:30,SplitRatio=0.7)
# returns the split in True/False values
split
summary(model)
ypred=predict(model,newdata=test_data)
summary(ypred)
ypred=predict(model,newdata=test_data)
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
train_dataset=subset(dataset,split==TRUE)
train_dataset
test_data=subset(dataset,split==FALSE)
# Build model
model=lm(formula=Salary~YearsExperience,data=train_dataset)
model # shows intercept and weight value
summary(model)
coef(model)
ypred=predict(model,newdata=test_data)
summary(ypred)
source('~/R/ML_by_R/Medicine_Expenses.R')
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
ggplot()+geom_point(aes(x=train_dataset$YearsExperience,y=train_dataset$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience),y=predict(model,newdata=train_dataset),color='blue')+
ggtitle('Salary vs Experience')+
xlab('Years of Experience')+
ylab('Salary')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
library(caTools)
library(ggplot2)
library(gmodels)
library(ggplot2)
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
source('~/R/ML_by_R/Salary_Linear_Regression.R')
ggplot()+geom_point(aes(x=test_data$YearsExperience,y=test_data$Salary),color='red')+
geom_line(aes(x=train_dataset$YearsExperience,y=predict(model,newdata=train_dataset)),color='blue')+
ggtitle('Salary as Experience')+
xlab('Years')+
ylab('Salary')
insurance=read.csv('insurance.txt')
str(insurance)
# Predicting Medical Expenses
str(insurance)
summary(insurance$charges)
hist(insurance$charges,col='blue')
table(insurance$region)
View(insurance)
# Exploring relationship b/w all variables by pearson correlation
# The diagonal here will be 1 becoz of variables covariance with itself and its variance is same
cor(insurance[c("age", "bmi", "children", "charges")])
# scatter plot
pairs(insurance[c("age", "bmi", "children", "charges")])
# scatter plot using psych
# It shows corelation and loess curve
pairs.panels(insurance[c("age", "bmi", "children", "charges")])
# model
# you can write . in place of all independent variables
model=lm(charges~age+children + bmi + sex + smoker + region, data = insurance)
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
# Convert bmi into binary
insurance$bmi2=ifelse(insurance$bmi>=30,1,0)
insurance$age2=insurance$age^2 # adding non-linear function
# Combined effect is known as interaction and it is represent by : b/w variables
# While using * b/w we use the 2 separate original variable as well as their interaction
# we tried to add interaction of bmi and smoker becoz their combined effect will decide the medical cost very effectively.
model=lm(charges~age + age2 + children + bmi + sex + smoker + region + bmi2*smoker,data=insurance)
model
# Now we got R2 value of 0.85 which is way better and higher than the previous one
summary(model)
The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
---
title: "R Notebook"
output: html_notebook
---
sdr1
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(tee))
sdr1
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(bt1))
sdr1
sdr1=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(bt1))
sdr1
sdr2=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
sdr2
# The formula used for regression tree is SDR
sdr1=sd(tee)-(length(at1)/length(tee)*sd(at1)+length(at2)/length(tee)*sd(at2))
sdr1
sdr2=sd(tee)-(length(bt1)/length(tee)*sd(bt1)+length(bt2)/length(tee)*sd(bt2))
sdr2
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('whitewines.txt')
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
hist(data$quality)
hist(data$quality,col='light blue')
hist(data$quality,col='sky blue')
install.packages("rpart")
install.packages("rpart.plot")
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
model
model=rpart(quality~.,w_train)
library(rpart)
model=rpart(quality~.,w_train)
model
# The nodes having * can be used for predictions
summary(model)
rpart.plot(model)
library(rpart.plot)
rpart.plot(model)
rpart.plot(model,digits=4,type=3,fallen.leaves=TRUE,extra = 101)
pred=predict(model,w_test)
summary(pred)
summary(data$quality)
cor(pred,data$quality)
cor(pred,data$quality)
cor(pred,w_test$quality)
summary(w_test$quality)
cor(pred,w_test$quality)
# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
return mean(abs(act-pred))}
# So we need to check mean absolute error
MAE=function(act,pred){return mean(abs(act-pred))}
# So we need to check mean absolute error
MAE<-function(act,pred){
return mean(abs(act-pred))
}
MAE <- function(actual, predicted) {
mean(abs(actual - predicted))
}
# So we need to check mean absolute error
MAE<-function(act,pred){
mean(abs(act-pred))
}
# So we need to check mean absolute error
MAE=function(act,pred){
mean(abs(act-pred))
}
MAE(w_test$quality,pred)
library(RWeka)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
# Using Regression models at regression trees
model=M5P(quality~.,w_train)
summary(model)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
# sd has more sdr value so regression tree will use sdr2 first
data=read.csv('white_wines.txt')
str(data)
summary(data)
hist(data$quality,col='sky blue')
w_train=data[1:3750,]
w_test=data[3750:length(data$quality),]
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
model=MSP(quality~.,data=w_train)
# Using Regression models at regression trees
model=M5P(quality~.,data=w_train)
summary(model)
model
pred=predict(model,w_test)
summary(pred)
# Using Regression models at regression trees
m.m5p=M5P(quality~.,data=w_train)
summary(m.m5p)
m.m5p
pred=predict(m.m5p,w_test)
m.m5p <- M5P(quality ~ ., data = w_train)
m.m5p
summary(m.m5p)
install.packages("neuralnet")
library(neuralnet)
data=read.csv('concrete.txt')
str(data)
summary(data)
(x-min(x))/(max(x)-min(x))
norm=function(x){
(x-min(x))/(max(x)-min(x))
}
concrete=as.data.frame(lapply(norm,data))
concrete
concrete=as.data.frame(lapply(data,norm))
concrete
data=na.omit(data)
str(data)
summary(data)
concrete=as.data.frame(lapply(data,norm))
concrete
summary(concrete)
v=length(concrete)/0.8
v=length(concrete$cement)/0.8
split=split(length(concrete$cement),v)
set.seed(12345)
v=as.integer(length(concrete$cement)/0.8)
split=split(length(concrete$cement),v)
split
sp=sample(length(concrete$cement),v)
v=as.integer(length(concrete$cement)*0.8)
sp=sample(length(concrete$cement),v)
split
sp
# Generating Random Numbers
set.seed(1)
sd=sample(1000,900)
sd
train=concrete[sd:,]
train=concrete[sp:,]
v=as.integer(length(concrete$cement)*0.8)
sp=sample(length(concrete$cement),v)
sp
train=concrete[sp:,]
train=concrete[sp:]
train=concrete[c(sp):,]
train=concrete[sd,]
length(sp)
train=concrete[sd,]
test=concrete[-sd,]
v=as.integer(length(concrete$cement)*0.75)
sp=sample(length(concrete$cement),v)
length(sp)
train=concrete[sd,]
test=concrete[-sd,]
test=concrete[-sd,]
str(test)
train=concrete[sp,]
test=concrete[-sp,]
str(test)
summary(concrete)
library(neuralnet)
model=neuralnet(strength~.,train)
plot(model)
model=neuralnet(strength~.,train,hidden=1)
model
summary(model)
str(model)
model # Shows each weight and intercept/bias
pred=compute(model,test)
pred
plot(pred)
summary(pred)
str(pred)
cor(pred,test$strength)
pred=compute(model,test)
cor(pred,test$strength)
pred$neurons
pred$net.result
cor(pred$net.result,test$strength)
# Make neural network with 5 hidden layers
model=neuralnet(strength~.,train,hidden=5)
summary(model)
model
plot(model)
pred=compute(model,test)
cor(pred,test$strength)
cor(pred$net.result,test$strength)
# Make neural network with 5 hidden nodes
model=neuralnet(strength~.,train,hidden=4)
model
plot(model)
pred=compute(model,test)
cor(pred$net.result,test$strength)
plot(model)
pred=compute(model,test)
cor(pred$net.result,test$strength)
letters <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml11/letterdata.csv")
letters <- read.csv("letters_data")
letters <- read.csv("letters_data.txt")
str(letters)
library(e1071)
letters <- read.csv("letters_data.txt")
str(letters)
install.packages("ksvm")
install.packages("kernlab")
library(kernlab)
train=letters[1:16000,]
test=letters[16001:20000,]
model=ksvm(letter~.,train,kernel='vanilladot',C=1)
model
summary(model)
l=test$letter
pred=predict(model,test)
head(test$letter)
table(pred,test$letter)
# For checking confusion matrix
v=table(pred,test$letter)
table=table(v)
table
# For checking confusion matrix
v=pred==test$letter
table=table(v)
table
prop.table(table)
model=ksvm(letter~.,train,kernel='rbfdot',C=1)
pred=predict(model,test)
arg=pred==test$letter
prop.table(arg)
prop.table(table(arg)
prop.table(table(arg)
prop.table(table(arg))
