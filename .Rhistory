str(usedcars)
usedcars<-read.csv('Usedcars.txt')
str(usedcars)
# Categorical variable like Year, model etc are examined using table rather than summary
table(usedcars$year)
table(usedcars$model)
table(usedcars$color)
# since table give us the count of each categorical variable
# We can also find proportion or percentage of each variable in the data by using prop.table
model_table=tabel(usedcars$model)
# since table give us the count of each categorical variable
# We can also find proportion or percentage of each variable in the data by using prop.table
model_table=table(usedcars$model)
prop.table(model_table)
prop.table(usedcars$price)
usedcars$price
prop.table(usedcars$year)
# Always use prop.table() on the table of categorical variable otherwise proportion of every single data point will be calculated
color_table<-table(usedcars$year)
color_table<-prop.table(color_table)*100
round(color_table,1)
# Always use prop.table() on the table of categorical variable otherwise proportion of every single data point will be calculated
color_table<-table(usedcars$color)
color_table<-prop.table(color_table)*100
round(color_table,1)
round(color_table,digits=2)
# Always use prop.table() on the table of categorical variable otherwise proportion of every single data point will be calculated
color_table<-table(usedcars$year)
color_table<-prop.table(color_table)*100
round(color_table,digits=2)
mode(color_table)
mode(usedcars$year) # mode() in R doesn't give mode but type of data i.e. 'Numeric','list',etc.
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)')
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',c='blue')
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)')
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col='blue')
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('blue','black'))
hist(usedcars$price,main='Histogram of Prices',ylab='Prices ($)',breaks=15,col='green')
hist(usedcars$mileage,main='Histogram of Mileage',ylab='Mileage (Km)',breaks=30,col='light blue')
hist(usedcars$mileage,main='Histogram of Mileage',ylab='Mileage (Km)',breaks=30,col='sky blue')
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col='blue','black')
c(
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('blue','black'))
hist(usedcars$mileage,main='Histogram of Mileage',ylab='Mileage (Km)',breaks=30,col='sky blue')
hist(usedcars$mileage,main='Histogram of Mileage',ylab='Mileage (Km)',breaks=30,col='sky blue')
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('blue','black'))
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('blue','black'),legend.text)
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('blue','black'),legend.text=c('Price','Mileage'))
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('blue','black'),args.legend=c('Price','Mileage'))
# Scatter-Plot :
plot(x=usedcars$mileage,y=usedcars$price,xlab='Mileage (Km)',ylab='Price ($)',col=c('sky blue'))
install.packages("gmodels")
library(gmodels)
conservative<-usedcars$color %in% c('Black','Silver','White','Gray')
conservative
conservative<-usedcars$color in c('Black','Silver','White','Gray')
table(conservative)
# checking the relation between the model and the color
CrossTable(x=usedcars$model,y=conservative)
# stringsAsFactors=FALSE let the strings data columns as character type
# While stringsAsFactors=TRUE makes the strings data columns as Factor type
# Default value of stringsSFactors=TRUE
usedcars<-read.csv('Usedcars.txt')
str(usedcars)
usedcars$price[1:9]
usedcars$color
summary(usedcars)
summary(usedcars[c('price','color')])
usedcars[1]
usedcars[1,2]
usedcars[1:2]
usedcars[1:2][1]
usedcars[1:2][1,2]
usedcars[1:2][1,]
usedcars[1:2][,2]
usedcars[1:2][4,]
usedcars[1,]
usedcars[1,][,c(3,5)]
lapply(5,usedcars[1])
lapply(usedcars[1],5)
x=function(x){x}
lapply(usedcars[1],)
lapply(usedcars[1],x)
x=function(x){5*x}
lapply(usedcars[1],x)
lapply(usedcars[1,],x)
# lapply() applies only a function to some given data
b=lapply(usedcars[1,],x)
b
as.data.frame(b)
subject_name <- c("John Doe", "Jane Doe", "Steve Graves")
temperature <- c(98.1, 98.6, 101.4)
flu_status <- c(FALSE, FALSE, TRUE)
temperature[1:2]
temperature[-2] # deletes the 2nd argument
temperature[-1] # deletes the 1st argument
temperature[-(length(temperature)-1)]
temperature[c(flu_status)]
# Factors :-  Used for Categorical Variables
gender=factor(c('Male','Female','Male'))
gender
blood <- factor(c("O", "AB", "A"),levels = c("A", "B", "AB", "O"),ordered = TRUE)
blood<'AB'
# Lists :-
b=list(full_name=subject_name[1],temperature=temperature[1],flu=flu_status[1],gender=gender[1],blood=blood[1])
b
library(class)
wdbc=read.csv('Cancer',sep=',',stringsAsFactors=FALSE)
wdbc=read.csv('Cancer.data',sep=',',stringsAsFactors=FALSE)
str(wdbc)
wdbc=read.table('Cancer.data',sep=',',stringsAsFactors=FALSE)
str(wdbc)
wdbc=read.table('Cancer.csv',sep=',',stringsAsFactors=FALSE)
str(wdbc)
wdbc=read.table('Cancer.csv',stringsAsFactors=FALSE)
wdbc=read.table('Cancer.csv',stringsAsFactors=FALSE)
str(wdbc)
wdbc=read.table('Cancer.csv',sep=',',stringsAsFactors=FALSE)
str(wdbc)
wdbc=read.table('Cancer.txt',sep=',',stringsAsFactors=FALSE)
wdbc=read.table('Cancer',sep=',',stringsAsFactors=FALSE)
wdbc=read.table('Cancer',sep=',',stringsAsFactors=FALSE,header = False)
wdbc=read.table('Cancer.csv',sep=',',stringsAsFactors=FALSE,header = False)
wdbc=read.csv('Cancer.csv',sep=',',stringsAsFactors=FALSE,header = False)
wdbc=read.csv('Cancer.csv',sep=',',stringsAsFactors=FALSE,header = FALSE)
str(wdbc)
wdbc=read.csv('Cancer.csv',sep=',',stringsAsFactors=FALSE,header = TRUE)
str(wdbc)
wdbc=read.csv('Cancer.csv',sep=',',stringsAsFactors=FALSE,header = TRUE)
str(wdbc)
wdbc[1]
# Remove the id col
wdbc=wdbc[-1]
wdbc[1]
summary(wdbc[1])
summary(wdbc)
table(wdbc$diagnosis)
# Change the class into Factor Vector
wdbc$diagnosis<-factor(wdbc$diagnosis)
wdbc$diagnosis
wdbc
str(wdbc)
# Change the class into Factor Vector
wdbc$diagnosis<-factor(wdbc$diagnosis,levels=c('B','M'),labels=c('Bengin','Malignant'))
str(wdbc)
table(wdbc[1])
table(wdbc$diagnosis)
prop.table(table(wdbc))
prop.table(table(wdbc[1]))
prop.table(table(wdbc[1]))*100
# Use the normalization by creating a function
norm=function(x){(x-min(x))/max(x)-min(x)}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# create Training and Testing data
length(wdbc_norm)
# create Training and Testing data
length(wdbc_norm[1])
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
str(wdbc_norm)
str(wdbc_train)
# Create Training data
wdbc_train=wdbc_norm[1:469,]
wdbc_train_label=wdbc[1:469,1]
str(wdbc_train)
wdbc_norm=as.data.frame(lapply(wdbc[2:32], norm))
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){((x-min(x))/max(x)-min(x))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){(x-min(x))/(max(x)-min(x))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){ return (x-min(x))/(max(x)-min(x))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){
return (x-min(x))/(max(x)-min(x))
}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
return ((x-min(x))/(max(x)-min(x)))
# Use the normalization by creating a function
norm=function(x){
return ((x-min(x))/(max(x)-min(x)))
}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
wdbc_norm=as.data.frame(lapply(wdbc[2:31], normalize))
summary(wdbc_norm)
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
# Use the normalization by creating a function
norm=function(x){ ((x-min(x))/(max(x)-min(x)))}
wdbc_norm=as.data.frame(lapply(wdbc[2:31], norm))
summary(wdbc_norm)
wdbc_test_label=wdbc[470:569,]
str(wdbc_train)
# Create Training data
wdbc_train=wdbc_norm[1:469,]
wdbc_train_label=wdbc[1:469,1]
str(wdbc_train)
wdbc_train_label=wdbc[1:469,1]
str(wdbc_train)
# Create Testing data
wdbc_test=wdbc_norm[470:569,]
wdbc_test_label=wdbc[470:569,]
str(wdbc_test_label)
wdbc_test_label=wdbc[470:569,1]
str(wdbc_test_label)
wdbc_test_label
str(wdbc_test)
library(gmodels)
# Training model :-
# Here, k value is the sq. root of the total no. of training samples
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=21)
# Evaluating model performances :-
CrossTable(x=wdbc_test_labels,y=wdbc_test_pred,chisq=FALSE)
# Evaluating model performances :-
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,chisq=FALSE)
# Evaluating model performances :-
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,chisq=TRUE)
# Evaluating model performances :-
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# Improving Model Performance :-
# Now use the standardization where mean is 0
wdbc_z=as.data.frame(scale(wdbc[2:32]))
# Improving Model Performance :-
# Now use the standardization where mean is 0
wdbc_z=as.data.frame(scale(wdbc[2:31]))
str(wdbc_z)
summary(wdbc_z)
# Now again create Traing and Testing data
wdbc_train=wdbc_z[1:469,]
wdbc_test=wdbc_z[470:569,]
str(wdbc_test)
# Build model :-
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=21)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# Now again create Traing and Testing data
wdbc_train=wdbc_z[1:469,]
wdbc_test=wdbc_z[470:569,]
str(wdbc_test)
# Build model :-
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=21)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# The performance didn't chnage but by changng k the the accuracy is different
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=2)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# The performance didn't chnage but by changng k the the accuracy is different
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=15)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
# The performance didn't chnage but by changng k the the accuracy is different
wdbc_test_pred=knn(wdbc_train,wdbc_test,cl=wdbc_train_label,k=10)
CrossTable(x=wdbc_test_label,y=wdbc_test_pred,prop.chisq=FALSE)
corpus=read.csv('spam.csv',sep=',')
str(csv)
str(corpus)
corpus=read.csv('spam.csv',sep=',',headers=TRUE)
corpus=read.csv('spam.csv',sep=',',header=TRUE)
str(corpus)
corpus=read.csv('spam.csv',sep=',')
str(corpus$v1)
prop.table(table(corpus$v1))
str(corpus$v2)
View(corpus)
lenght(corpus$v2)
length(corpus$v2)
length(corpus$x1)
raw=read.csv('spam.csv',sep=',')
length(raw$v2)
str(craw)
prop.table(table(raw$v1))
str(raw)
prop.table(table(raw$v1))
length(raw$X.1)
rwa$x.1[2,]
raw$x.1[2,]
corpus=VCorpus(VectorSource(raw$v2))
install.packages("tm")
VectorSource
prop.table(table(raw$v1))
source('~/.active-rstudio-document', encoding = 'UTF-8')
print(corpus[1:2])
# for displaying text use as.character
as.character(corpus[1])
# For applying it to all use lapply
corpus=lapply(corpus,as.character)
# refers to a volatile corpus—volatile as it is stored in memory as opposed to being stored on disk
# we use the VectorSource() reader function to create a source object from the existing raw$v2 vector.
corpus=VCorpus(VectorSource(raw$v2))
print(corpus[1:2])
# for displaying text use as.character
as.character(corpus[1])
# For applying it to all use lapply
corpus=lapply(corpus,as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
# for displaying text use as.character
as.character(corpus[[1]])
# For applying it to all use lapply
corpus=lapply(corpus,as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
# for displaying text use as.character
as.character(corpus[[1]])
# For applying it to all use lapply
lapply(corpus[1:5],as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
clean_corpus[1:2]
# refers to a volatile corpus—volatile as it is stored in memory as opposed to being stored on disk
# we use the VectorSource() reader function to create a source object from the existing raw$v2 vector.
corpus=VCorpus(VectorSource(raw$v2))
print(corpus[1:2])
# for displaying text use as.character
as.character(corpus[[1]])
# For applying it to all use lapply
lapply(corpus[1:5],as.character)
# we use tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.
clean_corpus=tm_map(corpus,content_transformer(tolower))
clean_corpus[1:2]
as.character(clean_corpus[1:2])
as.character(clean_corpus[[1:2]])
as.character(clean_corpus[[1]])
install.packages("SnowballC")
clean_corpus=tm_map(clean_corpus,removePunctuation)
detach("package:NLP", unload = TRUE)
clean_corpus=tm_map(clean_corpus,removePunctuation)
clean_corpus=tm_map(clean_corpus,stemDocument)
wordStem(c("learn", "learned", "learning", "learns"))
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
as.character(clean_corpus[[1]])
# remove words is a function of tm while stopwords is the collection of most commonly used words
clean_corpus=tm_map(clean_corpus,removeWords,stopwords())
as.character(clean_corpus[[1]])
clean_corpus=tm_map(clean_corpus,removePunctuation)
wordStem(c("learn", "learned", "learning", "learns"))
clean_corpus=tm_map(clean_corpus,stemDocument)
as.character(clean_corpus[[1]])
clean_corpus=tm_map(clean_corpus,stripWhiteSpace)
clean_corpus=tm_map(clean_corpus,stripWhitespace)
as.character(clean_corpus[[1]])
# Tokenization
sm_dtm=DocumentTermMatrix(clean_corpus)
str(sm_dtm)
str(sms_dtm2)
# If we didn't have done the preprocessing then we should prepare data by the following method
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,removeNumbers = TRUE,stopwords = TRUE,removePunctuation = TRUE,stemming = TRUE))
# If we didn't have done the preprocessing then we should prepare data by the following method
sms_dtm2 <- DocumentTermMatrix(corpus, control = list(tolower = TRUE,removeNumbers = TRUE,stopwords = TRUE,removePunctuation = TRUE,stemming = TRUE))
str(sms_dtm2)
sms_dtm
# Tokenization
sms_dtm=DocumentTermMatrix(clean_corpus)
str(sms_dtm)
str(sms_dtm2)
sms_dtm
sms_dtm2
sms_dtm2[1]
sms_dtm[1]
sms_dtm2[1]
sms_dtm[1,]
# Creating Training and Testing Data
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$v1
sms_train_labels <- raw[1:4169, ]$v1
sms_test_labels <- raw[4170:5559, ]$v1
sms_test_labels
table(sms_test_labels)
prop.table(sms_test_labels)
prop.table(factor(sms_test_labels))
prop.table(table(sms_test_labels))
install.packages("wordcloud")
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=70,random.order=FALSE)
# Creating WordCloud
wordcloud(clean_corpus,min.freq=50,random.order=FALSE)
spam=subset(raw$v2,type=='spam')
spam=subset(raw$v2,type=='Spam')
spam=subset(raw,type=='Spam')
spam=subset(raw,type=='spam')
spam=subset(raw,type=='spam')
spam=subset(raw,v1=='spam')
wordcloud(spam,min.freq=40)
wordcloud(spam$v2,min.freq=40)
wordcloud(spam$v2,min.freq=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham,min.freq=40,scale=c(3,0.5))
wordcloud(ham$v2,min.freq=40,scale=c(3,0.5))
wordcloud(spam$v2,max.words=40,scale=c(3,0.5))
ham=subset(raw,v1=='ham')
wordcloud(ham$v2,max.words=40,scale=c(3,0.5))
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
source('~/R/ML_by_R/Spam_SMS.R', encoding = 'UTF-8')
# Tokenization
sms_dtm=DocumentTermMatrix(clean_corpus)
str(sms_dtm)
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
# Create Dataset of frequent terms
# findFreqTerms takes a dtm  and return a vector of all specified frequent terms
sd=findFreqTerms(sms_dtm,5)
sd[1:5]
sms_train=sms_dtm_train[,sd]
sms_test=sms_dtm_test[,sd]
# Naive Bayes typically trains on data with categorical values
corp=function(x){ x=ifelse(x>0,'yes','no')}
# apply() can be used on either rows or columns
# MARGIN=1 means row-wise and 2 means column-wise
sms_train=apply(sms_train,MARGIN=2,corp)
sms_test=apply(sms_test,MARGIN = 2,corp)
install.packages("e1071")
library(e1071)
library(gmodels)
# Building Model :-
# Default Laplace is also 0
nb=naiveBayes(sms_train,sms_train_labels,laplace=0)
# Evaluating Model:-
pred=predict(nb,sms_test)
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'))
CrossTable(pred,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=1)
pred1=predict(nb,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
# Using Laplace :
nb1=naiveBayes(sms_train,sms_train_labels,laplace=3)
pred1=predict(nb,sms_test)
CrossTable(pred1,sms_test_labels,prop.chisq=FALSE,dnn=c('predicted','actual'),prop.t = FALSE)
sd[1:10]
length(sd)
install.packages("C50")
library(C50)
credit=read.csv('credit.data')
str(credit)
credit=read.csv('credit.data',header=TRUE)
str(credit)
credit=read.csv('credit.txt',header=TRUE)
str(credit)
x=seq(0,1,by=0.01)
x
curve(x,-x*log2(x)-(1-x)*log2(1-x))
type(c)
type(x)
typeof(x)
typeof(x)
curve(x,-x*log2(x)-(1-x)*log2(1-x))
x=c(seq(0,1,by=0.01))
typeof(x)
v=-x*log2(x)-(1-x)*log2(1-x)
print(v)
curve(x,v)
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy", lwd = 4)
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy")
rm()
rm(list=ls())
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=3,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=2,to=2.7,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1.5,n=5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1.5,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
# Entropy curve
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
library(C50)
credit=read.csv('credit.txt')
str(credit)
# Entropy curve
# Default values of from and to are also 0 and 1
curve(-x * log2(x) - (1 - x) * log2(1 - x),from=0,to=1,col = "red", xlab = "x", ylab = "Entropy",lwd=5)
